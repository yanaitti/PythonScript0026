{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ドライブ D のボリューム ラベルがありません。\n",
      " ボリューム シリアル番号は 3417-05D0 です\n",
      "\n",
      " D:\\PythonTraining\\Signate\\【練習問題】銀行の顧客ターゲティング のディレクトリ\n",
      "\n",
      "2020/09/26  13:38    <DIR>          .\n",
      "2020/09/26  13:38    <DIR>          ..\n",
      "2020/09/26  13:36    <DIR>          .ipynb_checkpoints\n",
      "2020/09/05  13:25         6,180,211 20200905_001.ipynb\n",
      "2020/09/26  13:26           482,395 20200926_001.csv\n",
      "2020/09/26  13:27           356,160 20200926_training1.ipynb\n",
      "2020/09/26  11:40         2,591,582 mytrain.csv\n",
      "2020/09/05  09:57           205,890 submit_sample.csv\n",
      "2020/09/05  09:57         1,523,536 test.csv\n",
      "2020/09/05  09:57         2,345,067 train.csv\n",
      "2020/09/26  13:38               555 Untitled.ipynb\n",
      "               8 個のファイル          13,685,396 バイト\n",
      "               3 個のディレクトリ  958,166,077,440 バイトの空き領域\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataの読み込み\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submit_df = pd.read_csv('submit_sample.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ、テストデータがわかるようにダミーの目的変数を代入\n",
    "test_df['y']=-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練データ、テストデータを結合\n",
    "all_df = pd.concat([train_df,test_df])\n",
    "del train_df,test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1756</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>3</td>\n",
       "      <td>apr</td>\n",
       "      <td>939</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1443</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>18</td>\n",
       "      <td>feb</td>\n",
       "      <td>172</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>436</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>13</td>\n",
       "      <td>apr</td>\n",
       "      <td>567</td>\n",
       "      <td>1</td>\n",
       "      <td>595</td>\n",
       "      <td>2</td>\n",
       "      <td>failure</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>474</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>25</td>\n",
       "      <td>jan</td>\n",
       "      <td>423</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>354</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>30</td>\n",
       "      <td>apr</td>\n",
       "      <td>502</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18078</th>\n",
       "      <td>18079</td>\n",
       "      <td>30</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>32</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6</td>\n",
       "      <td>may</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18079</th>\n",
       "      <td>18080</td>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1557</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>5</td>\n",
       "      <td>feb</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18080</th>\n",
       "      <td>18081</td>\n",
       "      <td>33</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1713</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>8</td>\n",
       "      <td>may</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18081</th>\n",
       "      <td>18082</td>\n",
       "      <td>37</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>-251</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>12</td>\n",
       "      <td>may</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>370</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18082</th>\n",
       "      <td>18083</td>\n",
       "      <td>34</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>56</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>aug</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45211 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  age           job  marital  education default  balance housing  \\\n",
       "0          1   39   blue-collar  married  secondary      no     1756     yes   \n",
       "1          2   51  entrepreneur  married    primary      no     1443      no   \n",
       "2          3   36    management   single   tertiary      no      436      no   \n",
       "3          4   63       retired  married  secondary      no      474      no   \n",
       "4          5   31    management   single   tertiary      no      354      no   \n",
       "...      ...  ...           ...      ...        ...     ...      ...     ...   \n",
       "18078  18079   30    management  married   tertiary      no       32     yes   \n",
       "18079  18080   35    management  married   tertiary      no     1557     yes   \n",
       "18080  18081   33     housemaid  married    primary      no     1713     yes   \n",
       "18081  18082   37   blue-collar  married  secondary      no     -251     yes   \n",
       "18082  18083   34    technician  married  secondary      no       56      no   \n",
       "\n",
       "      loan   contact  day month  duration  campaign  pdays  previous poutcome  \\\n",
       "0       no  cellular    3   apr       939         1     -1         0  unknown   \n",
       "1       no  cellular   18   feb       172        10     -1         0  unknown   \n",
       "2       no  cellular   13   apr       567         1    595         2  failure   \n",
       "3       no  cellular   25   jan       423         1     -1         0  unknown   \n",
       "4       no  cellular   30   apr       502         1      9         2  success   \n",
       "...    ...       ...  ...   ...       ...       ...    ...       ...      ...   \n",
       "18078   no   unknown    6   may       122         3     -1         0  unknown   \n",
       "18079  yes  cellular    5   feb       225         1    268         1  failure   \n",
       "18080   no   unknown    8   may        22         1     -1         0  unknown   \n",
       "18081   no  cellular   12   may       146         3    370         1  failure   \n",
       "18082   no  cellular   19   aug        91         2     -1         0  unknown   \n",
       "\n",
       "         y  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "...    ...  \n",
       "18078 -999  \n",
       "18079 -999  \n",
       "18080 -999  \n",
       "18081 -999  \n",
       "18082 -999  \n",
       "\n",
       "[45211 rows x 18 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリカラムの前処理\n",
    "categorical_features = ['job', 'marital', 'education','default','housing','loan','contact','month','poutcome']\n",
    "for col in categorical_features:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(all_df[col])\n",
    "    lbl.transform(all_df[col])\n",
    "    all_df[col]=lbl.transform(all_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>939</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>172</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>1</td>\n",
       "      <td>595</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>474</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>423</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>502</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18078</th>\n",
       "      <td>18079</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18079</th>\n",
       "      <td>18080</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1557</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18080</th>\n",
       "      <td>18081</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1713</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18081</th>\n",
       "      <td>18082</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-251</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>370</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18082</th>\n",
       "      <td>18083</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45211 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  age  job  marital  education  default  balance  housing  loan  \\\n",
       "0          1   39    1        1          1        0     1756        1     0   \n",
       "1          2   51    2        1          0        0     1443        0     0   \n",
       "2          3   36    4        2          2        0      436        0     0   \n",
       "3          4   63    5        1          1        0      474        0     0   \n",
       "4          5   31    4        2          2        0      354        0     0   \n",
       "...      ...  ...  ...      ...        ...      ...      ...      ...   ...   \n",
       "18078  18079   30    4        1          2        0       32        1     0   \n",
       "18079  18080   35    4        1          2        0     1557        1     1   \n",
       "18080  18081   33    3        1          0        0     1713        1     0   \n",
       "18081  18082   37    1        1          1        0     -251        1     0   \n",
       "18082  18083   34    9        1          1        0       56        0     0   \n",
       "\n",
       "       contact  day  month  duration  campaign  pdays  previous  poutcome    y  \n",
       "0            0    3      0       939         1     -1         0         3    1  \n",
       "1            0   18      3       172        10     -1         0         3    1  \n",
       "2            0   13      0       567         1    595         2         0    1  \n",
       "3            0   25      4       423         1     -1         0         3    1  \n",
       "4            0   30      0       502         1      9         2         2    1  \n",
       "...        ...  ...    ...       ...       ...    ...       ...       ...  ...  \n",
       "18078        2    6      8       122         3     -1         0         3 -999  \n",
       "18079        0    5      3       225         1    268         1         0 -999  \n",
       "18080        2    8      8        22         1     -1         0         3 -999  \n",
       "18081        0   12      8       146         3    370         1         0 -999  \n",
       "18082        0   19      1        91         2     -1         0         3 -999  \n",
       "\n",
       "[45211 rows x 18 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ、テストデータの分割\n",
    "train_df = all_df[all_df['y']!=-999]\n",
    "test_df = all_df[all_df['y']==-999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['y']\n",
    "X_train = train_df.drop(['y','id'], axis=1)\n",
    "X_test = test_df.drop(['y','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データからデータを分割\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2222, number of negative: 16767\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 986\n",
      "[LightGBM] [Info] Number of data points in the train set: 18989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117015 -> initscore=-2.021005\n",
      "[LightGBM] [Info] Start training from score -2.021005\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.232564\tvalid_1's binary_logloss: 0.237691\n",
      "[20]\ttraining's binary_logloss: 0.199925\tvalid_1's binary_logloss: 0.210887\n",
      "[30]\ttraining's binary_logloss: 0.184378\tvalid_1's binary_logloss: 0.20327\n",
      "[40]\ttraining's binary_logloss: 0.173768\tvalid_1's binary_logloss: 0.19966\n",
      "[50]\ttraining's binary_logloss: 0.165284\tvalid_1's binary_logloss: 0.197542\n",
      "[60]\ttraining's binary_logloss: 0.158623\tvalid_1's binary_logloss: 0.197489\n",
      "[70]\ttraining's binary_logloss: 0.153175\tvalid_1's binary_logloss: 0.19707\n",
      "[80]\ttraining's binary_logloss: 0.147729\tvalid_1's binary_logloss: 0.19753\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttraining's binary_logloss: 0.152644\tvalid_1's binary_logloss: 0.196925\n"
     ]
    }
   ],
   "source": [
    "# 使用モデルはLGB（パラメータチューニング無）\n",
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, categorical_feature=categorical_features)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary'\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params, lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_eval],\n",
    "    verbose_eval=10,\n",
    "    num_boost_round=1000,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54799381, 0.53038736, 0.00069622, ..., 0.00187813, 0.00988003,\n",
       "       0.00640652])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上位ランカー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_X_train = X_train.copy()\n",
    "org_y_train = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18989, 16), (18989,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_X_train.shape, org_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230247\tvalid_1's binary_logloss: 0.243308\n",
      "[20]\ttraining's binary_logloss: 0.195441\tvalid_1's binary_logloss: 0.219242\n",
      "[30]\ttraining's binary_logloss: 0.177864\tvalid_1's binary_logloss: 0.212391\n",
      "[40]\ttraining's binary_logloss: 0.164663\tvalid_1's binary_logloss: 0.210063\n",
      "[50]\ttraining's binary_logloss: 0.15476\tvalid_1's binary_logloss: 0.20936\n",
      "[60]\ttraining's binary_logloss: 0.147011\tvalid_1's binary_logloss: 0.209832\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's binary_logloss: 0.15476\tvalid_1's binary_logloss: 0.20936\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22967\tvalid_1's binary_logloss: 0.244812\n",
      "[20]\ttraining's binary_logloss: 0.195631\tvalid_1's binary_logloss: 0.220836\n",
      "[30]\ttraining's binary_logloss: 0.178415\tvalid_1's binary_logloss: 0.21271\n",
      "[40]\ttraining's binary_logloss: 0.166377\tvalid_1's binary_logloss: 0.20959\n",
      "[50]\ttraining's binary_logloss: 0.156005\tvalid_1's binary_logloss: 0.207875\n",
      "[60]\ttraining's binary_logloss: 0.148254\tvalid_1's binary_logloss: 0.208938\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's binary_logloss: 0.156005\tvalid_1's binary_logloss: 0.207875\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230141\tvalid_1's binary_logloss: 0.243802\n",
      "[20]\ttraining's binary_logloss: 0.196605\tvalid_1's binary_logloss: 0.221632\n",
      "[30]\ttraining's binary_logloss: 0.178645\tvalid_1's binary_logloss: 0.214307\n",
      "[40]\ttraining's binary_logloss: 0.166149\tvalid_1's binary_logloss: 0.210961\n",
      "[50]\ttraining's binary_logloss: 0.155735\tvalid_1's binary_logloss: 0.209871\n",
      "[60]\ttraining's binary_logloss: 0.148399\tvalid_1's binary_logloss: 0.209607\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's binary_logloss: 0.149165\tvalid_1's binary_logloss: 0.209537\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.23278\tvalid_1's binary_logloss: 0.241379\n",
      "[20]\ttraining's binary_logloss: 0.200381\tvalid_1's binary_logloss: 0.215153\n",
      "[30]\ttraining's binary_logloss: 0.181236\tvalid_1's binary_logloss: 0.204823\n",
      "[40]\ttraining's binary_logloss: 0.168466\tvalid_1's binary_logloss: 0.201262\n",
      "[50]\ttraining's binary_logloss: 0.15929\tvalid_1's binary_logloss: 0.199123\n",
      "[60]\ttraining's binary_logloss: 0.151048\tvalid_1's binary_logloss: 0.199001\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's binary_logloss: 0.15563\tvalid_1's binary_logloss: 0.198776\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231063\tvalid_1's binary_logloss: 0.238499\n",
      "[20]\ttraining's binary_logloss: 0.197923\tvalid_1's binary_logloss: 0.2152\n",
      "[30]\ttraining's binary_logloss: 0.180176\tvalid_1's binary_logloss: 0.206286\n",
      "[40]\ttraining's binary_logloss: 0.168103\tvalid_1's binary_logloss: 0.203099\n",
      "[50]\ttraining's binary_logloss: 0.158455\tvalid_1's binary_logloss: 0.203205\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's binary_logloss: 0.165045\tvalid_1's binary_logloss: 0.202801\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.2291\tvalid_1's binary_logloss: 0.244778\n",
      "[20]\ttraining's binary_logloss: 0.195365\tvalid_1's binary_logloss: 0.221507\n",
      "[30]\ttraining's binary_logloss: 0.178138\tvalid_1's binary_logloss: 0.213403\n",
      "[40]\ttraining's binary_logloss: 0.165213\tvalid_1's binary_logloss: 0.209572\n",
      "[50]\ttraining's binary_logloss: 0.1553\tvalid_1's binary_logloss: 0.20826\n",
      "[60]\ttraining's binary_logloss: 0.147479\tvalid_1's binary_logloss: 0.20769\n",
      "[70]\ttraining's binary_logloss: 0.14025\tvalid_1's binary_logloss: 0.206882\n",
      "[80]\ttraining's binary_logloss: 0.134024\tvalid_1's binary_logloss: 0.207446\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's binary_logloss: 0.14025\tvalid_1's binary_logloss: 0.206882\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 976\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.233421\tvalid_1's binary_logloss: 0.238471\n",
      "[20]\ttraining's binary_logloss: 0.200503\tvalid_1's binary_logloss: 0.214699\n",
      "[30]\ttraining's binary_logloss: 0.182413\tvalid_1's binary_logloss: 0.206119\n",
      "[40]\ttraining's binary_logloss: 0.168605\tvalid_1's binary_logloss: 0.201016\n",
      "[50]\ttraining's binary_logloss: 0.158915\tvalid_1's binary_logloss: 0.200136\n",
      "[60]\ttraining's binary_logloss: 0.150167\tvalid_1's binary_logloss: 0.200021\n",
      "[70]\ttraining's binary_logloss: 0.143214\tvalid_1's binary_logloss: 0.200248\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's binary_logloss: 0.149254\tvalid_1's binary_logloss: 0.199706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231186\tvalid_1's binary_logloss: 0.24392\n",
      "[20]\ttraining's binary_logloss: 0.197315\tvalid_1's binary_logloss: 0.219648\n",
      "[30]\ttraining's binary_logloss: 0.179299\tvalid_1's binary_logloss: 0.212144\n",
      "[40]\ttraining's binary_logloss: 0.166957\tvalid_1's binary_logloss: 0.209218\n",
      "[50]\ttraining's binary_logloss: 0.15785\tvalid_1's binary_logloss: 0.207312\n",
      "[60]\ttraining's binary_logloss: 0.149171\tvalid_1's binary_logloss: 0.207107\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's binary_logloss: 0.155839\tvalid_1's binary_logloss: 0.206859\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230406\tvalid_1's binary_logloss: 0.241602\n",
      "[20]\ttraining's binary_logloss: 0.197225\tvalid_1's binary_logloss: 0.218559\n",
      "[30]\ttraining's binary_logloss: 0.179297\tvalid_1's binary_logloss: 0.211324\n",
      "[40]\ttraining's binary_logloss: 0.166435\tvalid_1's binary_logloss: 0.208626\n",
      "[50]\ttraining's binary_logloss: 0.156084\tvalid_1's binary_logloss: 0.207252\n",
      "[60]\ttraining's binary_logloss: 0.148223\tvalid_1's binary_logloss: 0.207585\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's binary_logloss: 0.151972\tvalid_1's binary_logloss: 0.20711\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231962\tvalid_1's binary_logloss: 0.243457\n",
      "[20]\ttraining's binary_logloss: 0.198262\tvalid_1's binary_logloss: 0.219753\n",
      "[30]\ttraining's binary_logloss: 0.180284\tvalid_1's binary_logloss: 0.212449\n",
      "[40]\ttraining's binary_logloss: 0.167787\tvalid_1's binary_logloss: 0.20936\n",
      "[50]\ttraining's binary_logloss: 0.157834\tvalid_1's binary_logloss: 0.208016\n",
      "[60]\ttraining's binary_logloss: 0.149778\tvalid_1's binary_logloss: 0.208356\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's binary_logloss: 0.154567\tvalid_1's binary_logloss: 0.207735\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231466\tvalid_1's binary_logloss: 0.239491\n",
      "[20]\ttraining's binary_logloss: 0.198511\tvalid_1's binary_logloss: 0.214595\n",
      "[30]\ttraining's binary_logloss: 0.180122\tvalid_1's binary_logloss: 0.206796\n",
      "[40]\ttraining's binary_logloss: 0.16768\tvalid_1's binary_logloss: 0.20345\n",
      "[50]\ttraining's binary_logloss: 0.158512\tvalid_1's binary_logloss: 0.202143\n",
      "[60]\ttraining's binary_logloss: 0.150314\tvalid_1's binary_logloss: 0.201297\n",
      "[70]\ttraining's binary_logloss: 0.143616\tvalid_1's binary_logloss: 0.20254\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's binary_logloss: 0.150314\tvalid_1's binary_logloss: 0.201297\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230904\tvalid_1's binary_logloss: 0.241439\n",
      "[20]\ttraining's binary_logloss: 0.196772\tvalid_1's binary_logloss: 0.216956\n",
      "[30]\ttraining's binary_logloss: 0.179095\tvalid_1's binary_logloss: 0.208975\n",
      "[40]\ttraining's binary_logloss: 0.165999\tvalid_1's binary_logloss: 0.205983\n",
      "[50]\ttraining's binary_logloss: 0.155847\tvalid_1's binary_logloss: 0.204646\n",
      "[60]\ttraining's binary_logloss: 0.146919\tvalid_1's binary_logloss: 0.205003\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's binary_logloss: 0.14843\tvalid_1's binary_logloss: 0.204586\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230501\tvalid_1's binary_logloss: 0.24536\n",
      "[20]\ttraining's binary_logloss: 0.196922\tvalid_1's binary_logloss: 0.22208\n",
      "[30]\ttraining's binary_logloss: 0.179004\tvalid_1's binary_logloss: 0.214559\n",
      "[40]\ttraining's binary_logloss: 0.16634\tvalid_1's binary_logloss: 0.211605\n",
      "[50]\ttraining's binary_logloss: 0.156778\tvalid_1's binary_logloss: 0.2107\n",
      "[60]\ttraining's binary_logloss: 0.147809\tvalid_1's binary_logloss: 0.208982\n",
      "[70]\ttraining's binary_logloss: 0.140655\tvalid_1's binary_logloss: 0.209223\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's binary_logloss: 0.145417\tvalid_1's binary_logloss: 0.208784\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229165\tvalid_1's binary_logloss: 0.247768\n",
      "[20]\ttraining's binary_logloss: 0.194929\tvalid_1's binary_logloss: 0.225171\n",
      "[30]\ttraining's binary_logloss: 0.177147\tvalid_1's binary_logloss: 0.218552\n",
      "[40]\ttraining's binary_logloss: 0.16322\tvalid_1's binary_logloss: 0.215088\n",
      "[50]\ttraining's binary_logloss: 0.152944\tvalid_1's binary_logloss: 0.214187\n",
      "[60]\ttraining's binary_logloss: 0.14467\tvalid_1's binary_logloss: 0.213632\n",
      "[70]\ttraining's binary_logloss: 0.137927\tvalid_1's binary_logloss: 0.213794\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's binary_logloss: 0.143057\tvalid_1's binary_logloss: 0.213436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230577\tvalid_1's binary_logloss: 0.244048\n",
      "[20]\ttraining's binary_logloss: 0.197925\tvalid_1's binary_logloss: 0.221128\n",
      "[30]\ttraining's binary_logloss: 0.179118\tvalid_1's binary_logloss: 0.213183\n",
      "[40]\ttraining's binary_logloss: 0.165602\tvalid_1's binary_logloss: 0.209811\n",
      "[50]\ttraining's binary_logloss: 0.155747\tvalid_1's binary_logloss: 0.209661\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's binary_logloss: 0.156569\tvalid_1's binary_logloss: 0.209613\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.232335\tvalid_1's binary_logloss: 0.241373\n",
      "[20]\ttraining's binary_logloss: 0.198532\tvalid_1's binary_logloss: 0.215286\n",
      "[30]\ttraining's binary_logloss: 0.181495\tvalid_1's binary_logloss: 0.207285\n",
      "[40]\ttraining's binary_logloss: 0.169227\tvalid_1's binary_logloss: 0.204198\n",
      "[50]\ttraining's binary_logloss: 0.159155\tvalid_1's binary_logloss: 0.201667\n",
      "[60]\ttraining's binary_logloss: 0.15145\tvalid_1's binary_logloss: 0.200618\n",
      "[70]\ttraining's binary_logloss: 0.145043\tvalid_1's binary_logloss: 0.201154\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's binary_logloss: 0.149689\tvalid_1's binary_logloss: 0.200517\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.23365\tvalid_1's binary_logloss: 0.237089\n",
      "[20]\ttraining's binary_logloss: 0.199583\tvalid_1's binary_logloss: 0.211686\n",
      "[30]\ttraining's binary_logloss: 0.180816\tvalid_1's binary_logloss: 0.202958\n",
      "[40]\ttraining's binary_logloss: 0.168097\tvalid_1's binary_logloss: 0.200276\n",
      "[50]\ttraining's binary_logloss: 0.158196\tvalid_1's binary_logloss: 0.198786\n",
      "[60]\ttraining's binary_logloss: 0.149643\tvalid_1's binary_logloss: 0.198199\n",
      "[70]\ttraining's binary_logloss: 0.143184\tvalid_1's binary_logloss: 0.198513\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's binary_logloss: 0.148872\tvalid_1's binary_logloss: 0.198042\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230317\tvalid_1's binary_logloss: 0.244822\n",
      "[20]\ttraining's binary_logloss: 0.197447\tvalid_1's binary_logloss: 0.222802\n",
      "[30]\ttraining's binary_logloss: 0.179633\tvalid_1's binary_logloss: 0.215587\n",
      "[40]\ttraining's binary_logloss: 0.166719\tvalid_1's binary_logloss: 0.213103\n",
      "[50]\ttraining's binary_logloss: 0.156217\tvalid_1's binary_logloss: 0.211398\n",
      "[60]\ttraining's binary_logloss: 0.147875\tvalid_1's binary_logloss: 0.210488\n",
      "[70]\ttraining's binary_logloss: 0.140982\tvalid_1's binary_logloss: 0.21022\n",
      "[80]\ttraining's binary_logloss: 0.134416\tvalid_1's binary_logloss: 0.210508\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's binary_logloss: 0.13945\tvalid_1's binary_logloss: 0.209886\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231944\tvalid_1's binary_logloss: 0.239194\n",
      "[20]\ttraining's binary_logloss: 0.197256\tvalid_1's binary_logloss: 0.215369\n",
      "[30]\ttraining's binary_logloss: 0.179944\tvalid_1's binary_logloss: 0.207412\n",
      "[40]\ttraining's binary_logloss: 0.166497\tvalid_1's binary_logloss: 0.203251\n",
      "[50]\ttraining's binary_logloss: 0.157088\tvalid_1's binary_logloss: 0.203254\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's binary_logloss: 0.162591\tvalid_1's binary_logloss: 0.202932\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231154\tvalid_1's binary_logloss: 0.23833\n",
      "[20]\ttraining's binary_logloss: 0.198166\tvalid_1's binary_logloss: 0.214432\n",
      "[30]\ttraining's binary_logloss: 0.180685\tvalid_1's binary_logloss: 0.206371\n",
      "[40]\ttraining's binary_logloss: 0.168619\tvalid_1's binary_logloss: 0.202928\n",
      "[50]\ttraining's binary_logloss: 0.158809\tvalid_1's binary_logloss: 0.202726\n",
      "[60]\ttraining's binary_logloss: 0.151042\tvalid_1's binary_logloss: 0.203143\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's binary_logloss: 0.155253\tvalid_1's binary_logloss: 0.202522\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229275\tvalid_1's binary_logloss: 0.245185\n",
      "[20]\ttraining's binary_logloss: 0.19643\tvalid_1's binary_logloss: 0.224199\n",
      "[30]\ttraining's binary_logloss: 0.178154\tvalid_1's binary_logloss: 0.215676\n",
      "[40]\ttraining's binary_logloss: 0.166013\tvalid_1's binary_logloss: 0.211863\n",
      "[50]\ttraining's binary_logloss: 0.156407\tvalid_1's binary_logloss: 0.211285\n",
      "[60]\ttraining's binary_logloss: 0.147936\tvalid_1's binary_logloss: 0.210932\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's binary_logloss: 0.150219\tvalid_1's binary_logloss: 0.210384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228482\tvalid_1's binary_logloss: 0.245103\n",
      "[20]\ttraining's binary_logloss: 0.195012\tvalid_1's binary_logloss: 0.223834\n",
      "[30]\ttraining's binary_logloss: 0.176262\tvalid_1's binary_logloss: 0.216768\n",
      "[40]\ttraining's binary_logloss: 0.163273\tvalid_1's binary_logloss: 0.213576\n",
      "[50]\ttraining's binary_logloss: 0.15338\tvalid_1's binary_logloss: 0.212292\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's binary_logloss: 0.15502\tvalid_1's binary_logloss: 0.211805\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228037\tvalid_1's binary_logloss: 0.249375\n",
      "[20]\ttraining's binary_logloss: 0.194491\tvalid_1's binary_logloss: 0.227485\n",
      "[30]\ttraining's binary_logloss: 0.176441\tvalid_1's binary_logloss: 0.219095\n",
      "[40]\ttraining's binary_logloss: 0.163101\tvalid_1's binary_logloss: 0.216003\n",
      "[50]\ttraining's binary_logloss: 0.153473\tvalid_1's binary_logloss: 0.215295\n",
      "[60]\ttraining's binary_logloss: 0.145161\tvalid_1's binary_logloss: 0.214636\n",
      "[70]\ttraining's binary_logloss: 0.138363\tvalid_1's binary_logloss: 0.214451\n",
      "[80]\ttraining's binary_logloss: 0.132227\tvalid_1's binary_logloss: 0.214202\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's binary_logloss: 0.134654\tvalid_1's binary_logloss: 0.213985\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231338\tvalid_1's binary_logloss: 0.241267\n",
      "[20]\ttraining's binary_logloss: 0.198083\tvalid_1's binary_logloss: 0.217188\n",
      "[30]\ttraining's binary_logloss: 0.180486\tvalid_1's binary_logloss: 0.209128\n",
      "[40]\ttraining's binary_logloss: 0.16727\tvalid_1's binary_logloss: 0.20523\n",
      "[50]\ttraining's binary_logloss: 0.1577\tvalid_1's binary_logloss: 0.204062\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's binary_logloss: 0.159562\tvalid_1's binary_logloss: 0.203746\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.23012\tvalid_1's binary_logloss: 0.246656\n",
      "[20]\ttraining's binary_logloss: 0.198005\tvalid_1's binary_logloss: 0.225523\n",
      "[30]\ttraining's binary_logloss: 0.180671\tvalid_1's binary_logloss: 0.218756\n",
      "[40]\ttraining's binary_logloss: 0.167532\tvalid_1's binary_logloss: 0.215676\n",
      "[50]\ttraining's binary_logloss: 0.157067\tvalid_1's binary_logloss: 0.213357\n",
      "[60]\ttraining's binary_logloss: 0.1492\tvalid_1's binary_logloss: 0.212929\n",
      "[70]\ttraining's binary_logloss: 0.142447\tvalid_1's binary_logloss: 0.212934\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttraining's binary_logloss: 0.143784\tvalid_1's binary_logloss: 0.212753\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229722\tvalid_1's binary_logloss: 0.244864\n",
      "[20]\ttraining's binary_logloss: 0.196072\tvalid_1's binary_logloss: 0.222648\n",
      "[30]\ttraining's binary_logloss: 0.177761\tvalid_1's binary_logloss: 0.215136\n",
      "[40]\ttraining's binary_logloss: 0.16463\tvalid_1's binary_logloss: 0.212444\n",
      "[50]\ttraining's binary_logloss: 0.15502\tvalid_1's binary_logloss: 0.211444\n",
      "[60]\ttraining's binary_logloss: 0.147187\tvalid_1's binary_logloss: 0.211058\n",
      "[70]\ttraining's binary_logloss: 0.14008\tvalid_1's binary_logloss: 0.210667\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttraining's binary_logloss: 0.141399\tvalid_1's binary_logloss: 0.210555\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231869\tvalid_1's binary_logloss: 0.240042\n",
      "[20]\ttraining's binary_logloss: 0.19877\tvalid_1's binary_logloss: 0.21663\n",
      "[30]\ttraining's binary_logloss: 0.18097\tvalid_1's binary_logloss: 0.208684\n",
      "[40]\ttraining's binary_logloss: 0.168659\tvalid_1's binary_logloss: 0.20532\n",
      "[50]\ttraining's binary_logloss: 0.158686\tvalid_1's binary_logloss: 0.203794\n",
      "[60]\ttraining's binary_logloss: 0.150556\tvalid_1's binary_logloss: 0.203601\n",
      "[70]\ttraining's binary_logloss: 0.143746\tvalid_1's binary_logloss: 0.204059\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's binary_logloss: 0.147792\tvalid_1's binary_logloss: 0.203329\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229696\tvalid_1's binary_logloss: 0.24402\n",
      "[20]\ttraining's binary_logloss: 0.19572\tvalid_1's binary_logloss: 0.220069\n",
      "[30]\ttraining's binary_logloss: 0.178425\tvalid_1's binary_logloss: 0.211533\n",
      "[40]\ttraining's binary_logloss: 0.166283\tvalid_1's binary_logloss: 0.208434\n",
      "[50]\ttraining's binary_logloss: 0.155865\tvalid_1's binary_logloss: 0.206191\n",
      "[60]\ttraining's binary_logloss: 0.147771\tvalid_1's binary_logloss: 0.205815\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's binary_logloss: 0.148654\tvalid_1's binary_logloss: 0.205719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229237\tvalid_1's binary_logloss: 0.244634\n",
      "[20]\ttraining's binary_logloss: 0.194269\tvalid_1's binary_logloss: 0.220873\n",
      "[30]\ttraining's binary_logloss: 0.175971\tvalid_1's binary_logloss: 0.212957\n",
      "[40]\ttraining's binary_logloss: 0.163595\tvalid_1's binary_logloss: 0.209837\n",
      "[50]\ttraining's binary_logloss: 0.153951\tvalid_1's binary_logloss: 0.208411\n",
      "[60]\ttraining's binary_logloss: 0.146258\tvalid_1's binary_logloss: 0.208034\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's binary_logloss: 0.14773\tvalid_1's binary_logloss: 0.207957\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228347\tvalid_1's binary_logloss: 0.245669\n",
      "[20]\ttraining's binary_logloss: 0.194859\tvalid_1's binary_logloss: 0.220959\n",
      "[30]\ttraining's binary_logloss: 0.176972\tvalid_1's binary_logloss: 0.214083\n",
      "[40]\ttraining's binary_logloss: 0.164162\tvalid_1's binary_logloss: 0.211086\n",
      "[50]\ttraining's binary_logloss: 0.154313\tvalid_1's binary_logloss: 0.210354\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's binary_logloss: 0.155295\tvalid_1's binary_logloss: 0.210091\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231856\tvalid_1's binary_logloss: 0.240457\n",
      "[20]\ttraining's binary_logloss: 0.198359\tvalid_1's binary_logloss: 0.215959\n",
      "[30]\ttraining's binary_logloss: 0.181529\tvalid_1's binary_logloss: 0.208105\n",
      "[40]\ttraining's binary_logloss: 0.168092\tvalid_1's binary_logloss: 0.203557\n",
      "[50]\ttraining's binary_logloss: 0.157906\tvalid_1's binary_logloss: 0.201589\n",
      "[60]\ttraining's binary_logloss: 0.149984\tvalid_1's binary_logloss: 0.201345\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's binary_logloss: 0.154675\tvalid_1's binary_logloss: 0.201046\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230089\tvalid_1's binary_logloss: 0.241886\n",
      "[20]\ttraining's binary_logloss: 0.197758\tvalid_1's binary_logloss: 0.220266\n",
      "[30]\ttraining's binary_logloss: 0.179293\tvalid_1's binary_logloss: 0.212484\n",
      "[40]\ttraining's binary_logloss: 0.166431\tvalid_1's binary_logloss: 0.208628\n",
      "[50]\ttraining's binary_logloss: 0.15703\tvalid_1's binary_logloss: 0.208101\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.159509\tvalid_1's binary_logloss: 0.207956\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230922\tvalid_1's binary_logloss: 0.240705\n",
      "[20]\ttraining's binary_logloss: 0.19773\tvalid_1's binary_logloss: 0.216752\n",
      "[30]\ttraining's binary_logloss: 0.179322\tvalid_1's binary_logloss: 0.208534\n",
      "[40]\ttraining's binary_logloss: 0.16661\tvalid_1's binary_logloss: 0.20462\n",
      "[50]\ttraining's binary_logloss: 0.156456\tvalid_1's binary_logloss: 0.203466\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's binary_logloss: 0.158088\tvalid_1's binary_logloss: 0.203102\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22787\tvalid_1's binary_logloss: 0.248275\n",
      "[20]\ttraining's binary_logloss: 0.19369\tvalid_1's binary_logloss: 0.22562\n",
      "[30]\ttraining's binary_logloss: 0.176372\tvalid_1's binary_logloss: 0.217905\n",
      "[40]\ttraining's binary_logloss: 0.164474\tvalid_1's binary_logloss: 0.214546\n",
      "[50]\ttraining's binary_logloss: 0.1544\tvalid_1's binary_logloss: 0.213809\n",
      "[60]\ttraining's binary_logloss: 0.146408\tvalid_1's binary_logloss: 0.213795\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's binary_logloss: 0.148358\tvalid_1's binary_logloss: 0.213445\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231351\tvalid_1's binary_logloss: 0.238789\n",
      "[20]\ttraining's binary_logloss: 0.198132\tvalid_1's binary_logloss: 0.215281\n",
      "[30]\ttraining's binary_logloss: 0.1798\tvalid_1's binary_logloss: 0.207382\n",
      "[40]\ttraining's binary_logloss: 0.167281\tvalid_1's binary_logloss: 0.204213\n",
      "[50]\ttraining's binary_logloss: 0.157088\tvalid_1's binary_logloss: 0.203567\n",
      "[60]\ttraining's binary_logloss: 0.148966\tvalid_1's binary_logloss: 0.203011\n",
      "[70]\ttraining's binary_logloss: 0.141648\tvalid_1's binary_logloss: 0.202871\n",
      "[80]\ttraining's binary_logloss: 0.135208\tvalid_1's binary_logloss: 0.202622\n",
      "[90]\ttraining's binary_logloss: 0.129542\tvalid_1's binary_logloss: 0.202904\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's binary_logloss: 0.133737\tvalid_1's binary_logloss: 0.20227\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.226689\tvalid_1's binary_logloss: 0.245912\n",
      "[20]\ttraining's binary_logloss: 0.191541\tvalid_1's binary_logloss: 0.225069\n",
      "[30]\ttraining's binary_logloss: 0.173465\tvalid_1's binary_logloss: 0.218859\n",
      "[40]\ttraining's binary_logloss: 0.161356\tvalid_1's binary_logloss: 0.216875\n",
      "[50]\ttraining's binary_logloss: 0.151947\tvalid_1's binary_logloss: 0.216698\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's binary_logloss: 0.155514\tvalid_1's binary_logloss: 0.21607\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.232006\tvalid_1's binary_logloss: 0.237703\n",
      "[20]\ttraining's binary_logloss: 0.199302\tvalid_1's binary_logloss: 0.212808\n",
      "[30]\ttraining's binary_logloss: 0.181131\tvalid_1's binary_logloss: 0.203321\n",
      "[40]\ttraining's binary_logloss: 0.168545\tvalid_1's binary_logloss: 0.200438\n",
      "[50]\ttraining's binary_logloss: 0.158644\tvalid_1's binary_logloss: 0.198513\n",
      "[60]\ttraining's binary_logloss: 0.150198\tvalid_1's binary_logloss: 0.19773\n",
      "[70]\ttraining's binary_logloss: 0.142935\tvalid_1's binary_logloss: 0.198451\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's binary_logloss: 0.150198\tvalid_1's binary_logloss: 0.19773\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 983\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230943\tvalid_1's binary_logloss: 0.2425\n",
      "[20]\ttraining's binary_logloss: 0.197465\tvalid_1's binary_logloss: 0.219408\n",
      "[30]\ttraining's binary_logloss: 0.179282\tvalid_1's binary_logloss: 0.21126\n",
      "[40]\ttraining's binary_logloss: 0.166894\tvalid_1's binary_logloss: 0.207226\n",
      "[50]\ttraining's binary_logloss: 0.157526\tvalid_1's binary_logloss: 0.206298\n",
      "[60]\ttraining's binary_logloss: 0.149034\tvalid_1's binary_logloss: 0.20555\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's binary_logloss: 0.152756\tvalid_1's binary_logloss: 0.205525\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229063\tvalid_1's binary_logloss: 0.244006\n",
      "[20]\ttraining's binary_logloss: 0.194633\tvalid_1's binary_logloss: 0.22074\n",
      "[30]\ttraining's binary_logloss: 0.17647\tvalid_1's binary_logloss: 0.212945\n",
      "[40]\ttraining's binary_logloss: 0.163811\tvalid_1's binary_logloss: 0.210559\n",
      "[50]\ttraining's binary_logloss: 0.153257\tvalid_1's binary_logloss: 0.21031\n",
      "[60]\ttraining's binary_logloss: 0.144781\tvalid_1's binary_logloss: 0.21097\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's binary_logloss: 0.153257\tvalid_1's binary_logloss: 0.21031\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229475\tvalid_1's binary_logloss: 0.243271\n",
      "[20]\ttraining's binary_logloss: 0.195415\tvalid_1's binary_logloss: 0.22204\n",
      "[30]\ttraining's binary_logloss: 0.17606\tvalid_1's binary_logloss: 0.215034\n",
      "[40]\ttraining's binary_logloss: 0.163619\tvalid_1's binary_logloss: 0.214052\n",
      "[50]\ttraining's binary_logloss: 0.153409\tvalid_1's binary_logloss: 0.213043\n",
      "[60]\ttraining's binary_logloss: 0.145514\tvalid_1's binary_logloss: 0.213351\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's binary_logloss: 0.148172\tvalid_1's binary_logloss: 0.212723\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228091\tvalid_1's binary_logloss: 0.247239\n",
      "[20]\ttraining's binary_logloss: 0.194649\tvalid_1's binary_logloss: 0.224334\n",
      "[30]\ttraining's binary_logloss: 0.176015\tvalid_1's binary_logloss: 0.21749\n",
      "[40]\ttraining's binary_logloss: 0.163476\tvalid_1's binary_logloss: 0.215128\n",
      "[50]\ttraining's binary_logloss: 0.153668\tvalid_1's binary_logloss: 0.214976\n",
      "[60]\ttraining's binary_logloss: 0.145315\tvalid_1's binary_logloss: 0.215444\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's binary_logloss: 0.153668\tvalid_1's binary_logloss: 0.214976\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230011\tvalid_1's binary_logloss: 0.241535\n",
      "[20]\ttraining's binary_logloss: 0.196921\tvalid_1's binary_logloss: 0.218831\n",
      "[30]\ttraining's binary_logloss: 0.179121\tvalid_1's binary_logloss: 0.210692\n",
      "[40]\ttraining's binary_logloss: 0.166821\tvalid_1's binary_logloss: 0.207388\n",
      "[50]\ttraining's binary_logloss: 0.157627\tvalid_1's binary_logloss: 0.205449\n",
      "[60]\ttraining's binary_logloss: 0.14924\tvalid_1's binary_logloss: 0.204965\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's binary_logloss: 0.150651\tvalid_1's binary_logloss: 0.204683\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229276\tvalid_1's binary_logloss: 0.246413\n",
      "[20]\ttraining's binary_logloss: 0.196471\tvalid_1's binary_logloss: 0.225322\n",
      "[30]\ttraining's binary_logloss: 0.177746\tvalid_1's binary_logloss: 0.216586\n",
      "[40]\ttraining's binary_logloss: 0.165486\tvalid_1's binary_logloss: 0.213914\n",
      "[50]\ttraining's binary_logloss: 0.155546\tvalid_1's binary_logloss: 0.21315\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.158313\tvalid_1's binary_logloss: 0.212779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230468\tvalid_1's binary_logloss: 0.245376\n",
      "[20]\ttraining's binary_logloss: 0.195598\tvalid_1's binary_logloss: 0.221913\n",
      "[30]\ttraining's binary_logloss: 0.177296\tvalid_1's binary_logloss: 0.213193\n",
      "[40]\ttraining's binary_logloss: 0.165186\tvalid_1's binary_logloss: 0.210429\n",
      "[50]\ttraining's binary_logloss: 0.154843\tvalid_1's binary_logloss: 0.210316\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's binary_logloss: 0.16276\tvalid_1's binary_logloss: 0.209902\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229696\tvalid_1's binary_logloss: 0.244518\n",
      "[20]\ttraining's binary_logloss: 0.195201\tvalid_1's binary_logloss: 0.22162\n",
      "[30]\ttraining's binary_logloss: 0.177577\tvalid_1's binary_logloss: 0.214906\n",
      "[40]\ttraining's binary_logloss: 0.164593\tvalid_1's binary_logloss: 0.212026\n",
      "[50]\ttraining's binary_logloss: 0.155114\tvalid_1's binary_logloss: 0.211613\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.157835\tvalid_1's binary_logloss: 0.211448\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228894\tvalid_1's binary_logloss: 0.242125\n",
      "[20]\ttraining's binary_logloss: 0.196659\tvalid_1's binary_logloss: 0.220275\n",
      "[30]\ttraining's binary_logloss: 0.178793\tvalid_1's binary_logloss: 0.212531\n",
      "[40]\ttraining's binary_logloss: 0.165963\tvalid_1's binary_logloss: 0.209558\n",
      "[50]\ttraining's binary_logloss: 0.15627\tvalid_1's binary_logloss: 0.20904\n",
      "[60]\ttraining's binary_logloss: 0.147877\tvalid_1's binary_logloss: 0.208884\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's binary_logloss: 0.151063\tvalid_1's binary_logloss: 0.208474\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 976\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230877\tvalid_1's binary_logloss: 0.244436\n",
      "[20]\ttraining's binary_logloss: 0.198044\tvalid_1's binary_logloss: 0.219625\n",
      "[30]\ttraining's binary_logloss: 0.180608\tvalid_1's binary_logloss: 0.210513\n",
      "[40]\ttraining's binary_logloss: 0.168216\tvalid_1's binary_logloss: 0.206076\n",
      "[50]\ttraining's binary_logloss: 0.158971\tvalid_1's binary_logloss: 0.204776\n",
      "[60]\ttraining's binary_logloss: 0.150615\tvalid_1's binary_logloss: 0.20408\n",
      "[70]\ttraining's binary_logloss: 0.143381\tvalid_1's binary_logloss: 0.203297\n",
      "[80]\ttraining's binary_logloss: 0.137705\tvalid_1's binary_logloss: 0.20301\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttraining's binary_logloss: 0.139361\tvalid_1's binary_logloss: 0.202939\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.226867\tvalid_1's binary_logloss: 0.247715\n",
      "[20]\ttraining's binary_logloss: 0.192855\tvalid_1's binary_logloss: 0.228093\n",
      "[30]\ttraining's binary_logloss: 0.174317\tvalid_1's binary_logloss: 0.221181\n",
      "[40]\ttraining's binary_logloss: 0.161509\tvalid_1's binary_logloss: 0.219223\n",
      "[50]\ttraining's binary_logloss: 0.152351\tvalid_1's binary_logloss: 0.219235\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's binary_logloss: 0.157342\tvalid_1's binary_logloss: 0.218592\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230255\tvalid_1's binary_logloss: 0.243766\n",
      "[20]\ttraining's binary_logloss: 0.196115\tvalid_1's binary_logloss: 0.220905\n",
      "[30]\ttraining's binary_logloss: 0.178236\tvalid_1's binary_logloss: 0.212908\n",
      "[40]\ttraining's binary_logloss: 0.165931\tvalid_1's binary_logloss: 0.210591\n",
      "[50]\ttraining's binary_logloss: 0.156062\tvalid_1's binary_logloss: 0.209589\n",
      "[60]\ttraining's binary_logloss: 0.148281\tvalid_1's binary_logloss: 0.209098\n",
      "[70]\ttraining's binary_logloss: 0.142138\tvalid_1's binary_logloss: 0.208924\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttraining's binary_logloss: 0.14313\tvalid_1's binary_logloss: 0.208808\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231858\tvalid_1's binary_logloss: 0.242154\n",
      "[20]\ttraining's binary_logloss: 0.197799\tvalid_1's binary_logloss: 0.219058\n",
      "[30]\ttraining's binary_logloss: 0.179602\tvalid_1's binary_logloss: 0.211179\n",
      "[40]\ttraining's binary_logloss: 0.167164\tvalid_1's binary_logloss: 0.208627\n",
      "[50]\ttraining's binary_logloss: 0.157275\tvalid_1's binary_logloss: 0.207525\n",
      "[60]\ttraining's binary_logloss: 0.148658\tvalid_1's binary_logloss: 0.207759\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's binary_logloss: 0.154376\tvalid_1's binary_logloss: 0.207133\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttraining's binary_logloss: 0.231923\tvalid_1's binary_logloss: 0.242398\n",
      "[20]\ttraining's binary_logloss: 0.198456\tvalid_1's binary_logloss: 0.218088\n",
      "[30]\ttraining's binary_logloss: 0.179962\tvalid_1's binary_logloss: 0.209059\n",
      "[40]\ttraining's binary_logloss: 0.167325\tvalid_1's binary_logloss: 0.205342\n",
      "[50]\ttraining's binary_logloss: 0.157295\tvalid_1's binary_logloss: 0.204421\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's binary_logloss: 0.159254\tvalid_1's binary_logloss: 0.204252\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230292\tvalid_1's binary_logloss: 0.245085\n",
      "[20]\ttraining's binary_logloss: 0.196566\tvalid_1's binary_logloss: 0.220699\n",
      "[30]\ttraining's binary_logloss: 0.17911\tvalid_1's binary_logloss: 0.212754\n",
      "[40]\ttraining's binary_logloss: 0.166213\tvalid_1's binary_logloss: 0.209325\n",
      "[50]\ttraining's binary_logloss: 0.156735\tvalid_1's binary_logloss: 0.208213\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.159394\tvalid_1's binary_logloss: 0.207888\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22893\tvalid_1's binary_logloss: 0.245378\n",
      "[20]\ttraining's binary_logloss: 0.195707\tvalid_1's binary_logloss: 0.224249\n",
      "[30]\ttraining's binary_logloss: 0.177098\tvalid_1's binary_logloss: 0.215715\n",
      "[40]\ttraining's binary_logloss: 0.164064\tvalid_1's binary_logloss: 0.211515\n",
      "[50]\ttraining's binary_logloss: 0.15493\tvalid_1's binary_logloss: 0.210568\n",
      "[60]\ttraining's binary_logloss: 0.147408\tvalid_1's binary_logloss: 0.210744\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's binary_logloss: 0.152585\tvalid_1's binary_logloss: 0.210392\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22978\tvalid_1's binary_logloss: 0.243344\n",
      "[20]\ttraining's binary_logloss: 0.195949\tvalid_1's binary_logloss: 0.22049\n",
      "[30]\ttraining's binary_logloss: 0.178227\tvalid_1's binary_logloss: 0.21405\n",
      "[40]\ttraining's binary_logloss: 0.165554\tvalid_1's binary_logloss: 0.211206\n",
      "[50]\ttraining's binary_logloss: 0.15559\tvalid_1's binary_logloss: 0.210201\n",
      "[60]\ttraining's binary_logloss: 0.146785\tvalid_1's binary_logloss: 0.209749\n",
      "[70]\ttraining's binary_logloss: 0.13977\tvalid_1's binary_logloss: 0.209111\n",
      "[80]\ttraining's binary_logloss: 0.133416\tvalid_1's binary_logloss: 0.209501\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's binary_logloss: 0.138373\tvalid_1's binary_logloss: 0.20902\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229736\tvalid_1's binary_logloss: 0.243761\n",
      "[20]\ttraining's binary_logloss: 0.19563\tvalid_1's binary_logloss: 0.220389\n",
      "[30]\ttraining's binary_logloss: 0.176761\tvalid_1's binary_logloss: 0.212066\n",
      "[40]\ttraining's binary_logloss: 0.163195\tvalid_1's binary_logloss: 0.209455\n",
      "[50]\ttraining's binary_logloss: 0.153006\tvalid_1's binary_logloss: 0.208166\n",
      "[60]\ttraining's binary_logloss: 0.145285\tvalid_1's binary_logloss: 0.208429\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's binary_logloss: 0.15226\tvalid_1's binary_logloss: 0.208138\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.227841\tvalid_1's binary_logloss: 0.246067\n",
      "[20]\ttraining's binary_logloss: 0.193353\tvalid_1's binary_logloss: 0.221606\n",
      "[30]\ttraining's binary_logloss: 0.175543\tvalid_1's binary_logloss: 0.214735\n",
      "[40]\ttraining's binary_logloss: 0.163236\tvalid_1's binary_logloss: 0.213003\n",
      "[50]\ttraining's binary_logloss: 0.153646\tvalid_1's binary_logloss: 0.212378\n",
      "[60]\ttraining's binary_logloss: 0.145504\tvalid_1's binary_logloss: 0.211128\n",
      "[70]\ttraining's binary_logloss: 0.138086\tvalid_1's binary_logloss: 0.211181\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's binary_logloss: 0.145504\tvalid_1's binary_logloss: 0.211128\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230096\tvalid_1's binary_logloss: 0.245583\n",
      "[20]\ttraining's binary_logloss: 0.197145\tvalid_1's binary_logloss: 0.223019\n",
      "[30]\ttraining's binary_logloss: 0.179763\tvalid_1's binary_logloss: 0.21557\n",
      "[40]\ttraining's binary_logloss: 0.166806\tvalid_1's binary_logloss: 0.211687\n",
      "[50]\ttraining's binary_logloss: 0.157262\tvalid_1's binary_logloss: 0.210857\n",
      "[60]\ttraining's binary_logloss: 0.148626\tvalid_1's binary_logloss: 0.20998\n",
      "[70]\ttraining's binary_logloss: 0.141599\tvalid_1's binary_logloss: 0.210185\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's binary_logloss: 0.145125\tvalid_1's binary_logloss: 0.209703\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231804\tvalid_1's binary_logloss: 0.242836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttraining's binary_logloss: 0.197394\tvalid_1's binary_logloss: 0.218046\n",
      "[30]\ttraining's binary_logloss: 0.179802\tvalid_1's binary_logloss: 0.209482\n",
      "[40]\ttraining's binary_logloss: 0.166962\tvalid_1's binary_logloss: 0.205615\n",
      "[50]\ttraining's binary_logloss: 0.157722\tvalid_1's binary_logloss: 0.203778\n",
      "[60]\ttraining's binary_logloss: 0.149759\tvalid_1's binary_logloss: 0.203442\n",
      "[70]\ttraining's binary_logloss: 0.142589\tvalid_1's binary_logloss: 0.203447\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's binary_logloss: 0.145949\tvalid_1's binary_logloss: 0.203023\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231625\tvalid_1's binary_logloss: 0.242892\n",
      "[20]\ttraining's binary_logloss: 0.198164\tvalid_1's binary_logloss: 0.218744\n",
      "[30]\ttraining's binary_logloss: 0.180157\tvalid_1's binary_logloss: 0.210758\n",
      "[40]\ttraining's binary_logloss: 0.166951\tvalid_1's binary_logloss: 0.207752\n",
      "[50]\ttraining's binary_logloss: 0.156745\tvalid_1's binary_logloss: 0.20627\n",
      "[60]\ttraining's binary_logloss: 0.148282\tvalid_1's binary_logloss: 0.206068\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's binary_logloss: 0.148973\tvalid_1's binary_logloss: 0.205976\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229134\tvalid_1's binary_logloss: 0.2437\n",
      "[20]\ttraining's binary_logloss: 0.196156\tvalid_1's binary_logloss: 0.220091\n",
      "[30]\ttraining's binary_logloss: 0.177993\tvalid_1's binary_logloss: 0.212277\n",
      "[40]\ttraining's binary_logloss: 0.165936\tvalid_1's binary_logloss: 0.210321\n",
      "[50]\ttraining's binary_logloss: 0.155871\tvalid_1's binary_logloss: 0.208997\n",
      "[60]\ttraining's binary_logloss: 0.147997\tvalid_1's binary_logloss: 0.209308\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's binary_logloss: 0.154172\tvalid_1's binary_logloss: 0.208763\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229181\tvalid_1's binary_logloss: 0.24726\n",
      "[20]\ttraining's binary_logloss: 0.194293\tvalid_1's binary_logloss: 0.222836\n",
      "[30]\ttraining's binary_logloss: 0.176707\tvalid_1's binary_logloss: 0.215855\n",
      "[40]\ttraining's binary_logloss: 0.16462\tvalid_1's binary_logloss: 0.213122\n",
      "[50]\ttraining's binary_logloss: 0.15435\tvalid_1's binary_logloss: 0.212227\n",
      "[60]\ttraining's binary_logloss: 0.146483\tvalid_1's binary_logloss: 0.212935\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's binary_logloss: 0.153471\tvalid_1's binary_logloss: 0.212177\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.227162\tvalid_1's binary_logloss: 0.248075\n",
      "[20]\ttraining's binary_logloss: 0.192459\tvalid_1's binary_logloss: 0.227902\n",
      "[30]\ttraining's binary_logloss: 0.174327\tvalid_1's binary_logloss: 0.220603\n",
      "[40]\ttraining's binary_logloss: 0.161443\tvalid_1's binary_logloss: 0.217629\n",
      "[50]\ttraining's binary_logloss: 0.151891\tvalid_1's binary_logloss: 0.217034\n",
      "[60]\ttraining's binary_logloss: 0.144156\tvalid_1's binary_logloss: 0.217053\n",
      "[70]\ttraining's binary_logloss: 0.137582\tvalid_1's binary_logloss: 0.217689\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's binary_logloss: 0.142124\tvalid_1's binary_logloss: 0.216832\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229117\tvalid_1's binary_logloss: 0.242831\n",
      "[20]\ttraining's binary_logloss: 0.194905\tvalid_1's binary_logloss: 0.219876\n",
      "[30]\ttraining's binary_logloss: 0.176805\tvalid_1's binary_logloss: 0.212775\n",
      "[40]\ttraining's binary_logloss: 0.164628\tvalid_1's binary_logloss: 0.209931\n",
      "[50]\ttraining's binary_logloss: 0.155469\tvalid_1's binary_logloss: 0.208873\n",
      "[60]\ttraining's binary_logloss: 0.14668\tvalid_1's binary_logloss: 0.207831\n",
      "[70]\ttraining's binary_logloss: 0.139766\tvalid_1's binary_logloss: 0.207494\n",
      "[80]\ttraining's binary_logloss: 0.133392\tvalid_1's binary_logloss: 0.207892\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's binary_logloss: 0.138411\tvalid_1's binary_logloss: 0.207412\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230204\tvalid_1's binary_logloss: 0.243668\n",
      "[20]\ttraining's binary_logloss: 0.196028\tvalid_1's binary_logloss: 0.220071\n",
      "[30]\ttraining's binary_logloss: 0.178358\tvalid_1's binary_logloss: 0.212861\n",
      "[40]\ttraining's binary_logloss: 0.165759\tvalid_1's binary_logloss: 0.209868\n",
      "[50]\ttraining's binary_logloss: 0.155714\tvalid_1's binary_logloss: 0.208144\n",
      "[60]\ttraining's binary_logloss: 0.148241\tvalid_1's binary_logloss: 0.208062\n",
      "[70]\ttraining's binary_logloss: 0.141678\tvalid_1's binary_logloss: 0.207828\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttraining's binary_logloss: 0.143013\tvalid_1's binary_logloss: 0.207736\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231114\tvalid_1's binary_logloss: 0.243015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttraining's binary_logloss: 0.196905\tvalid_1's binary_logloss: 0.218322\n",
      "[30]\ttraining's binary_logloss: 0.178788\tvalid_1's binary_logloss: 0.208347\n",
      "[40]\ttraining's binary_logloss: 0.166442\tvalid_1's binary_logloss: 0.205216\n",
      "[50]\ttraining's binary_logloss: 0.156266\tvalid_1's binary_logloss: 0.203076\n",
      "[60]\ttraining's binary_logloss: 0.148323\tvalid_1's binary_logloss: 0.202708\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's binary_logloss: 0.149096\tvalid_1's binary_logloss: 0.202688\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 976\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.233118\tvalid_1's binary_logloss: 0.237755\n",
      "[20]\ttraining's binary_logloss: 0.200271\tvalid_1's binary_logloss: 0.214172\n",
      "[30]\ttraining's binary_logloss: 0.182623\tvalid_1's binary_logloss: 0.204862\n",
      "[40]\ttraining's binary_logloss: 0.169585\tvalid_1's binary_logloss: 0.200877\n",
      "[50]\ttraining's binary_logloss: 0.160101\tvalid_1's binary_logloss: 0.199464\n",
      "[60]\ttraining's binary_logloss: 0.15176\tvalid_1's binary_logloss: 0.199152\n",
      "[70]\ttraining's binary_logloss: 0.14437\tvalid_1's binary_logloss: 0.199546\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's binary_logloss: 0.150915\tvalid_1's binary_logloss: 0.199016\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.23062\tvalid_1's binary_logloss: 0.245853\n",
      "[20]\ttraining's binary_logloss: 0.197109\tvalid_1's binary_logloss: 0.22264\n",
      "[30]\ttraining's binary_logloss: 0.178632\tvalid_1's binary_logloss: 0.214054\n",
      "[40]\ttraining's binary_logloss: 0.166184\tvalid_1's binary_logloss: 0.212397\n",
      "[50]\ttraining's binary_logloss: 0.156676\tvalid_1's binary_logloss: 0.212051\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's binary_logloss: 0.16209\tvalid_1's binary_logloss: 0.211715\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22828\tvalid_1's binary_logloss: 0.247221\n",
      "[20]\ttraining's binary_logloss: 0.19429\tvalid_1's binary_logloss: 0.225436\n",
      "[30]\ttraining's binary_logloss: 0.176143\tvalid_1's binary_logloss: 0.2191\n",
      "[40]\ttraining's binary_logloss: 0.163805\tvalid_1's binary_logloss: 0.216894\n",
      "[50]\ttraining's binary_logloss: 0.154235\tvalid_1's binary_logloss: 0.216453\n",
      "[60]\ttraining's binary_logloss: 0.14611\tvalid_1's binary_logloss: 0.2161\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's binary_logloss: 0.151532\tvalid_1's binary_logloss: 0.216057\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.23193\tvalid_1's binary_logloss: 0.240158\n",
      "[20]\ttraining's binary_logloss: 0.198546\tvalid_1's binary_logloss: 0.216042\n",
      "[30]\ttraining's binary_logloss: 0.180363\tvalid_1's binary_logloss: 0.207623\n",
      "[40]\ttraining's binary_logloss: 0.167382\tvalid_1's binary_logloss: 0.203798\n",
      "[50]\ttraining's binary_logloss: 0.15718\tvalid_1's binary_logloss: 0.202571\n",
      "[60]\ttraining's binary_logloss: 0.14934\tvalid_1's binary_logloss: 0.20252\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's binary_logloss: 0.150115\tvalid_1's binary_logloss: 0.202372\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228882\tvalid_1's binary_logloss: 0.245205\n",
      "[20]\ttraining's binary_logloss: 0.19378\tvalid_1's binary_logloss: 0.223607\n",
      "[30]\ttraining's binary_logloss: 0.176211\tvalid_1's binary_logloss: 0.216856\n",
      "[40]\ttraining's binary_logloss: 0.163391\tvalid_1's binary_logloss: 0.214901\n",
      "[50]\ttraining's binary_logloss: 0.153475\tvalid_1's binary_logloss: 0.215156\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's binary_logloss: 0.159876\tvalid_1's binary_logloss: 0.214595\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230317\tvalid_1's binary_logloss: 0.24085\n",
      "[20]\ttraining's binary_logloss: 0.197305\tvalid_1's binary_logloss: 0.217818\n",
      "[30]\ttraining's binary_logloss: 0.179531\tvalid_1's binary_logloss: 0.21\n",
      "[40]\ttraining's binary_logloss: 0.166146\tvalid_1's binary_logloss: 0.207213\n",
      "[50]\ttraining's binary_logloss: 0.156486\tvalid_1's binary_logloss: 0.206898\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's binary_logloss: 0.159712\tvalid_1's binary_logloss: 0.206445\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228743\tvalid_1's binary_logloss: 0.244885\n",
      "[20]\ttraining's binary_logloss: 0.195523\tvalid_1's binary_logloss: 0.221207\n",
      "[30]\ttraining's binary_logloss: 0.177993\tvalid_1's binary_logloss: 0.212669\n",
      "[40]\ttraining's binary_logloss: 0.166032\tvalid_1's binary_logloss: 0.209837\n",
      "[50]\ttraining's binary_logloss: 0.155506\tvalid_1's binary_logloss: 0.208363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\ttraining's binary_logloss: 0.147986\tvalid_1's binary_logloss: 0.208105\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's binary_logloss: 0.151136\tvalid_1's binary_logloss: 0.207605\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231075\tvalid_1's binary_logloss: 0.244718\n",
      "[20]\ttraining's binary_logloss: 0.198234\tvalid_1's binary_logloss: 0.221331\n",
      "[30]\ttraining's binary_logloss: 0.180728\tvalid_1's binary_logloss: 0.212646\n",
      "[40]\ttraining's binary_logloss: 0.168453\tvalid_1's binary_logloss: 0.209897\n",
      "[50]\ttraining's binary_logloss: 0.158117\tvalid_1's binary_logloss: 0.208467\n",
      "[60]\ttraining's binary_logloss: 0.149423\tvalid_1's binary_logloss: 0.207413\n",
      "[70]\ttraining's binary_logloss: 0.142717\tvalid_1's binary_logloss: 0.20759\n",
      "[80]\ttraining's binary_logloss: 0.136513\tvalid_1's binary_logloss: 0.207476\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's binary_logloss: 0.141367\tvalid_1's binary_logloss: 0.207353\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228555\tvalid_1's binary_logloss: 0.244992\n",
      "[20]\ttraining's binary_logloss: 0.195608\tvalid_1's binary_logloss: 0.221521\n",
      "[30]\ttraining's binary_logloss: 0.177641\tvalid_1's binary_logloss: 0.213384\n",
      "[40]\ttraining's binary_logloss: 0.16455\tvalid_1's binary_logloss: 0.2101\n",
      "[50]\ttraining's binary_logloss: 0.155235\tvalid_1's binary_logloss: 0.209305\n",
      "[60]\ttraining's binary_logloss: 0.146615\tvalid_1's binary_logloss: 0.208564\n",
      "[70]\ttraining's binary_logloss: 0.1398\tvalid_1's binary_logloss: 0.208762\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's binary_logloss: 0.145535\tvalid_1's binary_logloss: 0.208205\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229591\tvalid_1's binary_logloss: 0.244632\n",
      "[20]\ttraining's binary_logloss: 0.195917\tvalid_1's binary_logloss: 0.221977\n",
      "[30]\ttraining's binary_logloss: 0.17753\tvalid_1's binary_logloss: 0.214638\n",
      "[40]\ttraining's binary_logloss: 0.164557\tvalid_1's binary_logloss: 0.212812\n",
      "[50]\ttraining's binary_logloss: 0.154852\tvalid_1's binary_logloss: 0.212323\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's binary_logloss: 0.155644\tvalid_1's binary_logloss: 0.212127\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231139\tvalid_1's binary_logloss: 0.245227\n",
      "[20]\ttraining's binary_logloss: 0.197408\tvalid_1's binary_logloss: 0.219336\n",
      "[30]\ttraining's binary_logloss: 0.179104\tvalid_1's binary_logloss: 0.211671\n",
      "[40]\ttraining's binary_logloss: 0.165926\tvalid_1's binary_logloss: 0.209246\n",
      "[50]\ttraining's binary_logloss: 0.15572\tvalid_1's binary_logloss: 0.208229\n",
      "[60]\ttraining's binary_logloss: 0.147571\tvalid_1's binary_logloss: 0.207645\n",
      "[70]\ttraining's binary_logloss: 0.140864\tvalid_1's binary_logloss: 0.208015\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's binary_logloss: 0.145024\tvalid_1's binary_logloss: 0.207598\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.232008\tvalid_1's binary_logloss: 0.24375\n",
      "[20]\ttraining's binary_logloss: 0.198884\tvalid_1's binary_logloss: 0.221669\n",
      "[30]\ttraining's binary_logloss: 0.180548\tvalid_1's binary_logloss: 0.2109\n",
      "[40]\ttraining's binary_logloss: 0.168096\tvalid_1's binary_logloss: 0.2089\n",
      "[50]\ttraining's binary_logloss: 0.158457\tvalid_1's binary_logloss: 0.208194\n",
      "[60]\ttraining's binary_logloss: 0.150423\tvalid_1's binary_logloss: 0.207103\n",
      "[70]\ttraining's binary_logloss: 0.143496\tvalid_1's binary_logloss: 0.20665\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's binary_logloss: 0.143981\tvalid_1's binary_logloss: 0.206392\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.232282\tvalid_1's binary_logloss: 0.241554\n",
      "[20]\ttraining's binary_logloss: 0.198796\tvalid_1's binary_logloss: 0.217467\n",
      "[30]\ttraining's binary_logloss: 0.181352\tvalid_1's binary_logloss: 0.209474\n",
      "[40]\ttraining's binary_logloss: 0.169159\tvalid_1's binary_logloss: 0.207456\n",
      "[50]\ttraining's binary_logloss: 0.158646\tvalid_1's binary_logloss: 0.206756\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's binary_logloss: 0.163648\tvalid_1's binary_logloss: 0.206419\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229109\tvalid_1's binary_logloss: 0.24561\n",
      "[20]\ttraining's binary_logloss: 0.195681\tvalid_1's binary_logloss: 0.224111\n",
      "[30]\ttraining's binary_logloss: 0.177124\tvalid_1's binary_logloss: 0.217102\n",
      "[40]\ttraining's binary_logloss: 0.164665\tvalid_1's binary_logloss: 0.212885\n",
      "[50]\ttraining's binary_logloss: 0.154601\tvalid_1's binary_logloss: 0.212631\n",
      "[60]\ttraining's binary_logloss: 0.146122\tvalid_1's binary_logloss: 0.211916\n",
      "[70]\ttraining's binary_logloss: 0.139597\tvalid_1's binary_logloss: 0.211658\n",
      "Early stopping, best iteration is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\ttraining's binary_logloss: 0.140205\tvalid_1's binary_logloss: 0.211644\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.233244\tvalid_1's binary_logloss: 0.23665\n",
      "[20]\ttraining's binary_logloss: 0.200242\tvalid_1's binary_logloss: 0.21168\n",
      "[30]\ttraining's binary_logloss: 0.181707\tvalid_1's binary_logloss: 0.202848\n",
      "[40]\ttraining's binary_logloss: 0.169198\tvalid_1's binary_logloss: 0.199957\n",
      "[50]\ttraining's binary_logloss: 0.159453\tvalid_1's binary_logloss: 0.198834\n",
      "[60]\ttraining's binary_logloss: 0.151571\tvalid_1's binary_logloss: 0.198183\n",
      "[70]\ttraining's binary_logloss: 0.144907\tvalid_1's binary_logloss: 0.198607\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's binary_logloss: 0.148504\tvalid_1's binary_logloss: 0.197977\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.227743\tvalid_1's binary_logloss: 0.245032\n",
      "[20]\ttraining's binary_logloss: 0.194362\tvalid_1's binary_logloss: 0.221363\n",
      "[30]\ttraining's binary_logloss: 0.176245\tvalid_1's binary_logloss: 0.214041\n",
      "[40]\ttraining's binary_logloss: 0.163674\tvalid_1's binary_logloss: 0.212246\n",
      "[50]\ttraining's binary_logloss: 0.154264\tvalid_1's binary_logloss: 0.211436\n",
      "[60]\ttraining's binary_logloss: 0.14664\tvalid_1's binary_logloss: 0.21183\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's binary_logloss: 0.151756\tvalid_1's binary_logloss: 0.210896\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22903\tvalid_1's binary_logloss: 0.244225\n",
      "[20]\ttraining's binary_logloss: 0.195743\tvalid_1's binary_logloss: 0.222074\n",
      "[30]\ttraining's binary_logloss: 0.177966\tvalid_1's binary_logloss: 0.214491\n",
      "[40]\ttraining's binary_logloss: 0.165554\tvalid_1's binary_logloss: 0.212275\n",
      "[50]\ttraining's binary_logloss: 0.155036\tvalid_1's binary_logloss: 0.211593\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.157949\tvalid_1's binary_logloss: 0.211312\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.228971\tvalid_1's binary_logloss: 0.243171\n",
      "[20]\ttraining's binary_logloss: 0.195041\tvalid_1's binary_logloss: 0.219886\n",
      "[30]\ttraining's binary_logloss: 0.178052\tvalid_1's binary_logloss: 0.214087\n",
      "[40]\ttraining's binary_logloss: 0.165229\tvalid_1's binary_logloss: 0.211333\n",
      "[50]\ttraining's binary_logloss: 0.155433\tvalid_1's binary_logloss: 0.211261\n",
      "[60]\ttraining's binary_logloss: 0.146079\tvalid_1's binary_logloss: 0.209628\n",
      "[70]\ttraining's binary_logloss: 0.139189\tvalid_1's binary_logloss: 0.209476\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's binary_logloss: 0.139679\tvalid_1's binary_logloss: 0.209298\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230339\tvalid_1's binary_logloss: 0.241791\n",
      "[20]\ttraining's binary_logloss: 0.197039\tvalid_1's binary_logloss: 0.219863\n",
      "[30]\ttraining's binary_logloss: 0.178712\tvalid_1's binary_logloss: 0.212817\n",
      "[40]\ttraining's binary_logloss: 0.166369\tvalid_1's binary_logloss: 0.210098\n",
      "[50]\ttraining's binary_logloss: 0.156419\tvalid_1's binary_logloss: 0.208995\n",
      "[60]\ttraining's binary_logloss: 0.148423\tvalid_1's binary_logloss: 0.208945\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's binary_logloss: 0.152926\tvalid_1's binary_logloss: 0.208751\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231213\tvalid_1's binary_logloss: 0.242876\n",
      "[20]\ttraining's binary_logloss: 0.196406\tvalid_1's binary_logloss: 0.218921\n",
      "[30]\ttraining's binary_logloss: 0.178301\tvalid_1's binary_logloss: 0.211154\n",
      "[40]\ttraining's binary_logloss: 0.165697\tvalid_1's binary_logloss: 0.208592\n",
      "[50]\ttraining's binary_logloss: 0.155613\tvalid_1's binary_logloss: 0.207344\n",
      "[60]\ttraining's binary_logloss: 0.147665\tvalid_1's binary_logloss: 0.206926\n",
      "[70]\ttraining's binary_logloss: 0.140985\tvalid_1's binary_logloss: 0.207257\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's binary_logloss: 0.14682\tvalid_1's binary_logloss: 0.206866\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229297\tvalid_1's binary_logloss: 0.245053\n",
      "[20]\ttraining's binary_logloss: 0.194263\tvalid_1's binary_logloss: 0.222947\n",
      "[30]\ttraining's binary_logloss: 0.176378\tvalid_1's binary_logloss: 0.216303\n",
      "[40]\ttraining's binary_logloss: 0.163523\tvalid_1's binary_logloss: 0.21367\n",
      "[50]\ttraining's binary_logloss: 0.1538\tvalid_1's binary_logloss: 0.213259\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.156353\tvalid_1's binary_logloss: 0.213146\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.232979\tvalid_1's binary_logloss: 0.237024\n",
      "[20]\ttraining's binary_logloss: 0.201075\tvalid_1's binary_logloss: 0.211276\n",
      "[30]\ttraining's binary_logloss: 0.182714\tvalid_1's binary_logloss: 0.200712\n",
      "[40]\ttraining's binary_logloss: 0.170075\tvalid_1's binary_logloss: 0.196774\n",
      "[50]\ttraining's binary_logloss: 0.160523\tvalid_1's binary_logloss: 0.195596\n",
      "[60]\ttraining's binary_logloss: 0.152613\tvalid_1's binary_logloss: 0.19527\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's binary_logloss: 0.155573\tvalid_1's binary_logloss: 0.19493\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230358\tvalid_1's binary_logloss: 0.240311\n",
      "[20]\ttraining's binary_logloss: 0.196333\tvalid_1's binary_logloss: 0.21632\n",
      "[30]\ttraining's binary_logloss: 0.178507\tvalid_1's binary_logloss: 0.209619\n",
      "[40]\ttraining's binary_logloss: 0.166387\tvalid_1's binary_logloss: 0.206651\n",
      "[50]\ttraining's binary_logloss: 0.156251\tvalid_1's binary_logloss: 0.206187\n",
      "[60]\ttraining's binary_logloss: 0.147562\tvalid_1's binary_logloss: 0.206382\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's binary_logloss: 0.151395\tvalid_1's binary_logloss: 0.206157\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.234138\tvalid_1's binary_logloss: 0.238543\n",
      "[20]\ttraining's binary_logloss: 0.200695\tvalid_1's binary_logloss: 0.212312\n",
      "[30]\ttraining's binary_logloss: 0.183023\tvalid_1's binary_logloss: 0.204057\n",
      "[40]\ttraining's binary_logloss: 0.169955\tvalid_1's binary_logloss: 0.200368\n",
      "[50]\ttraining's binary_logloss: 0.159281\tvalid_1's binary_logloss: 0.197969\n",
      "[60]\ttraining's binary_logloss: 0.150793\tvalid_1's binary_logloss: 0.197452\n",
      "[70]\ttraining's binary_logloss: 0.143657\tvalid_1's binary_logloss: 0.197302\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's binary_logloss: 0.148287\tvalid_1's binary_logloss: 0.197001\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22923\tvalid_1's binary_logloss: 0.246209\n",
      "[20]\ttraining's binary_logloss: 0.195268\tvalid_1's binary_logloss: 0.223294\n",
      "[30]\ttraining's binary_logloss: 0.176425\tvalid_1's binary_logloss: 0.215322\n",
      "[40]\ttraining's binary_logloss: 0.163244\tvalid_1's binary_logloss: 0.212749\n",
      "[50]\ttraining's binary_logloss: 0.153255\tvalid_1's binary_logloss: 0.212258\n",
      "[60]\ttraining's binary_logloss: 0.145061\tvalid_1's binary_logloss: 0.213126\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's binary_logloss: 0.152424\tvalid_1's binary_logloss: 0.212108\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.227837\tvalid_1's binary_logloss: 0.246698\n",
      "[20]\ttraining's binary_logloss: 0.194248\tvalid_1's binary_logloss: 0.223088\n",
      "[30]\ttraining's binary_logloss: 0.176824\tvalid_1's binary_logloss: 0.214455\n",
      "[40]\ttraining's binary_logloss: 0.164117\tvalid_1's binary_logloss: 0.210919\n",
      "[50]\ttraining's binary_logloss: 0.155184\tvalid_1's binary_logloss: 0.210125\n",
      "[60]\ttraining's binary_logloss: 0.147717\tvalid_1's binary_logloss: 0.210261\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's binary_logloss: 0.149941\tvalid_1's binary_logloss: 0.209979\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229652\tvalid_1's binary_logloss: 0.250483\n",
      "[20]\ttraining's binary_logloss: 0.194612\tvalid_1's binary_logloss: 0.227173\n",
      "[30]\ttraining's binary_logloss: 0.176526\tvalid_1's binary_logloss: 0.219591\n",
      "[40]\ttraining's binary_logloss: 0.163595\tvalid_1's binary_logloss: 0.215978\n",
      "[50]\ttraining's binary_logloss: 0.153961\tvalid_1's binary_logloss: 0.215283\n",
      "[60]\ttraining's binary_logloss: 0.14598\tvalid_1's binary_logloss: 0.215883\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's binary_logloss: 0.152035\tvalid_1's binary_logloss: 0.214977\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.233749\tvalid_1's binary_logloss: 0.243463\n",
      "[20]\ttraining's binary_logloss: 0.200414\tvalid_1's binary_logloss: 0.216941\n",
      "[30]\ttraining's binary_logloss: 0.182613\tvalid_1's binary_logloss: 0.206606\n",
      "[40]\ttraining's binary_logloss: 0.169533\tvalid_1's binary_logloss: 0.202211\n",
      "[50]\ttraining's binary_logloss: 0.159757\tvalid_1's binary_logloss: 0.201368\n",
      "[60]\ttraining's binary_logloss: 0.151574\tvalid_1's binary_logloss: 0.200618\n",
      "[70]\ttraining's binary_logloss: 0.144448\tvalid_1's binary_logloss: 0.20069\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's binary_logloss: 0.145029\tvalid_1's binary_logloss: 0.200539\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.232391\tvalid_1's binary_logloss: 0.238499\n",
      "[20]\ttraining's binary_logloss: 0.198396\tvalid_1's binary_logloss: 0.213501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\ttraining's binary_logloss: 0.180454\tvalid_1's binary_logloss: 0.205139\n",
      "[40]\ttraining's binary_logloss: 0.168162\tvalid_1's binary_logloss: 0.201962\n",
      "[50]\ttraining's binary_logloss: 0.158004\tvalid_1's binary_logloss: 0.200787\n",
      "[60]\ttraining's binary_logloss: 0.150199\tvalid_1's binary_logloss: 0.200376\n",
      "[70]\ttraining's binary_logloss: 0.143485\tvalid_1's binary_logloss: 0.201301\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's binary_logloss: 0.150199\tvalid_1's binary_logloss: 0.200376\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 979\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.23044\tvalid_1's binary_logloss: 0.241255\n",
      "[20]\ttraining's binary_logloss: 0.196567\tvalid_1's binary_logloss: 0.217295\n",
      "[30]\ttraining's binary_logloss: 0.178591\tvalid_1's binary_logloss: 0.211059\n",
      "[40]\ttraining's binary_logloss: 0.165887\tvalid_1's binary_logloss: 0.20861\n",
      "[50]\ttraining's binary_logloss: 0.156026\tvalid_1's binary_logloss: 0.207498\n",
      "[60]\ttraining's binary_logloss: 0.14796\tvalid_1's binary_logloss: 0.207674\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's binary_logloss: 0.14991\tvalid_1's binary_logloss: 0.207245\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230415\tvalid_1's binary_logloss: 0.243533\n",
      "[20]\ttraining's binary_logloss: 0.197293\tvalid_1's binary_logloss: 0.221488\n",
      "[30]\ttraining's binary_logloss: 0.179836\tvalid_1's binary_logloss: 0.212926\n",
      "[40]\ttraining's binary_logloss: 0.167658\tvalid_1's binary_logloss: 0.209528\n",
      "[50]\ttraining's binary_logloss: 0.158093\tvalid_1's binary_logloss: 0.207985\n",
      "[60]\ttraining's binary_logloss: 0.149805\tvalid_1's binary_logloss: 0.207034\n",
      "[70]\ttraining's binary_logloss: 0.14292\tvalid_1's binary_logloss: 0.207023\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's binary_logloss: 0.146652\tvalid_1's binary_logloss: 0.206712\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230432\tvalid_1's binary_logloss: 0.241091\n",
      "[20]\ttraining's binary_logloss: 0.196871\tvalid_1's binary_logloss: 0.217742\n",
      "[30]\ttraining's binary_logloss: 0.17859\tvalid_1's binary_logloss: 0.210219\n",
      "[40]\ttraining's binary_logloss: 0.166747\tvalid_1's binary_logloss: 0.20762\n",
      "[50]\ttraining's binary_logloss: 0.157415\tvalid_1's binary_logloss: 0.206817\n",
      "[60]\ttraining's binary_logloss: 0.149724\tvalid_1's binary_logloss: 0.206562\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's binary_logloss: 0.154026\tvalid_1's binary_logloss: 0.206315\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.229144\tvalid_1's binary_logloss: 0.24438\n",
      "[20]\ttraining's binary_logloss: 0.195959\tvalid_1's binary_logloss: 0.223801\n",
      "[30]\ttraining's binary_logloss: 0.177201\tvalid_1's binary_logloss: 0.216398\n",
      "[40]\ttraining's binary_logloss: 0.164864\tvalid_1's binary_logloss: 0.214645\n",
      "[50]\ttraining's binary_logloss: 0.155603\tvalid_1's binary_logloss: 0.214524\n",
      "[60]\ttraining's binary_logloss: 0.147377\tvalid_1's binary_logloss: 0.214289\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's binary_logloss: 0.14827\tvalid_1's binary_logloss: 0.214243\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.230164\tvalid_1's binary_logloss: 0.244457\n",
      "[20]\ttraining's binary_logloss: 0.197084\tvalid_1's binary_logloss: 0.220446\n",
      "[30]\ttraining's binary_logloss: 0.179343\tvalid_1's binary_logloss: 0.213702\n",
      "[40]\ttraining's binary_logloss: 0.165929\tvalid_1's binary_logloss: 0.209412\n",
      "[50]\ttraining's binary_logloss: 0.156126\tvalid_1's binary_logloss: 0.207986\n",
      "[60]\ttraining's binary_logloss: 0.148147\tvalid_1's binary_logloss: 0.207274\n",
      "[70]\ttraining's binary_logloss: 0.140894\tvalid_1's binary_logloss: 0.207045\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's binary_logloss: 0.145169\tvalid_1's binary_logloss: 0.206759\n",
      "[LightGBM] [Info] Number of positive: 1555, number of negative: 11737\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 13292, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116988 -> initscore=-2.021271\n",
      "[LightGBM] [Info] Start training from score -2.021271\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's binary_logloss: 0.231899\tvalid_1's binary_logloss: 0.240111\n",
      "[20]\ttraining's binary_logloss: 0.198014\tvalid_1's binary_logloss: 0.214641\n",
      "[30]\ttraining's binary_logloss: 0.180063\tvalid_1's binary_logloss: 0.205929\n",
      "[40]\ttraining's binary_logloss: 0.16714\tvalid_1's binary_logloss: 0.202138\n",
      "[50]\ttraining's binary_logloss: 0.157778\tvalid_1's binary_logloss: 0.2002\n",
      "[60]\ttraining's binary_logloss: 0.149687\tvalid_1's binary_logloss: 0.199651\n",
      "[70]\ttraining's binary_logloss: 0.142089\tvalid_1's binary_logloss: 0.199179\n",
      "[80]\ttraining's binary_logloss: 0.135712\tvalid_1's binary_logloss: 0.198707\n",
      "[90]\ttraining's binary_logloss: 0.129985\tvalid_1's binary_logloss: 0.199121\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's binary_logloss: 0.134931\tvalid_1's binary_logloss: 0.198682\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # 訓練データからデータを分割\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(org_X_train, org_y_train, test_size=0.3, random_state=i, stratify=org_y_train)\n",
    "\n",
    "    # 使用モデルはLGB（パラメータチューニング無）\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, categorical_feature=categorical_features)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary'\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params, lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_eval],\n",
    "        verbose_eval=10,\n",
    "        num_boost_round=1000,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    output_df[i] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.653842</td>\n",
       "      <td>0.627558</td>\n",
       "      <td>0.629079</td>\n",
       "      <td>0.613790</td>\n",
       "      <td>0.562826</td>\n",
       "      <td>0.670790</td>\n",
       "      <td>0.469940</td>\n",
       "      <td>0.618970</td>\n",
       "      <td>0.566463</td>\n",
       "      <td>0.575187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.647986</td>\n",
       "      <td>0.620316</td>\n",
       "      <td>0.509806</td>\n",
       "      <td>0.682011</td>\n",
       "      <td>0.542413</td>\n",
       "      <td>0.665957</td>\n",
       "      <td>0.617966</td>\n",
       "      <td>0.435486</td>\n",
       "      <td>0.570155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.694868</td>\n",
       "      <td>0.507771</td>\n",
       "      <td>0.541812</td>\n",
       "      <td>0.567020</td>\n",
       "      <td>0.522094</td>\n",
       "      <td>0.589407</td>\n",
       "      <td>0.581154</td>\n",
       "      <td>0.510897</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>0.601613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576314</td>\n",
       "      <td>0.634916</td>\n",
       "      <td>0.624657</td>\n",
       "      <td>0.567957</td>\n",
       "      <td>0.572659</td>\n",
       "      <td>0.734023</td>\n",
       "      <td>0.584220</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>0.655789</td>\n",
       "      <td>0.576540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.121461</td>\n",
       "      <td>0.136177</td>\n",
       "      <td>0.148118</td>\n",
       "      <td>0.108862</td>\n",
       "      <td>0.103429</td>\n",
       "      <td>0.097691</td>\n",
       "      <td>0.137395</td>\n",
       "      <td>0.111667</td>\n",
       "      <td>0.061546</td>\n",
       "      <td>0.133966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065710</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.080136</td>\n",
       "      <td>0.148322</td>\n",
       "      <td>0.063383</td>\n",
       "      <td>0.110875</td>\n",
       "      <td>0.140948</td>\n",
       "      <td>0.156615</td>\n",
       "      <td>0.109539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18078</th>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.002116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18079</th>\n",
       "      <td>0.071985</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>0.051640</td>\n",
       "      <td>0.071142</td>\n",
       "      <td>0.085822</td>\n",
       "      <td>0.058785</td>\n",
       "      <td>0.096057</td>\n",
       "      <td>0.041395</td>\n",
       "      <td>0.059228</td>\n",
       "      <td>0.051976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.092694</td>\n",
       "      <td>0.071620</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>0.044833</td>\n",
       "      <td>0.072139</td>\n",
       "      <td>0.110295</td>\n",
       "      <td>0.138452</td>\n",
       "      <td>0.031712</td>\n",
       "      <td>0.043701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18080</th>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026029</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18081</th>\n",
       "      <td>0.014867</td>\n",
       "      <td>0.015454</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.007845</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009719</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.005955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18082</th>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.006001</td>\n",
       "      <td>0.005673</td>\n",
       "      <td>0.043481</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>0.006118</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>0.005336</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.006931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18083 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.653842  0.627558  0.629079  0.613790  0.562826  0.670790  0.469940   \n",
       "1      0.694868  0.507771  0.541812  0.567020  0.522094  0.589407  0.581154   \n",
       "2      0.002482  0.003061  0.001429  0.001604  0.002926  0.000968  0.000759   \n",
       "3      0.004051  0.003310  0.001351  0.003004  0.004038  0.001817  0.001706   \n",
       "4      0.121461  0.136177  0.148118  0.108862  0.103429  0.097691  0.137395   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "18078  0.004162  0.003425  0.002220  0.004295  0.004689  0.001829  0.002389   \n",
       "18079  0.071985  0.037942  0.051640  0.071142  0.085822  0.058785  0.096057   \n",
       "18080  0.003432  0.002731  0.001609  0.001577  0.002926  0.001778  0.001499   \n",
       "18081  0.014867  0.015454  0.004032  0.007845  0.007688  0.006956  0.004494   \n",
       "18082  0.006755  0.006001  0.005673  0.043481  0.008788  0.006118  0.004685   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.618970  0.566463  0.575187  ...  0.642201  0.647986  0.620316   \n",
       "1      0.510897  0.586300  0.601613  ...  0.576314  0.634916  0.624657   \n",
       "2      0.001820  0.002605  0.001560  ...  0.001446  0.001227  0.001771   \n",
       "3      0.003152  0.002548  0.003006  ...  0.001916  0.002073  0.001177   \n",
       "4      0.111667  0.061546  0.133966  ...  0.065710  0.092515  0.051023   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "18078  0.004244  0.002894  0.004284  ...  0.002754  0.003617  0.002029   \n",
       "18079  0.041395  0.059228  0.051976  ...  0.044198  0.092694  0.071620   \n",
       "18080  0.002884  0.001794  0.002598  ...  0.026029  0.002155  0.001787   \n",
       "18081  0.007325  0.008433  0.009850  ...  0.009719  0.008601  0.009423   \n",
       "18082  0.013018  0.009384  0.005950  ...  0.006330  0.004804  0.006846   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.509806  0.682011  0.542413  0.665957  0.617966  0.435486  0.570155  \n",
       "1      0.567957  0.572659  0.734023  0.584220  0.574899  0.655789  0.576540  \n",
       "2      0.002025  0.001339  0.000814  0.001263  0.001418  0.000853  0.000339  \n",
       "3      0.001705  0.003355  0.001865  0.003705  0.001717  0.001395  0.001618  \n",
       "4      0.080136  0.148322  0.063383  0.110875  0.140948  0.156615  0.109539  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "18078  0.002928  0.004889  0.002682  0.003679  0.002788  0.003393  0.002116  \n",
       "18079  0.034818  0.044833  0.072139  0.110295  0.138452  0.031712  0.043701  \n",
       "18080  0.001662  0.002233  0.001168  0.001815  0.001700  0.001128  0.000794  \n",
       "18081  0.006718  0.006409  0.006017  0.007959  0.006593  0.006973  0.005955  \n",
       "18082  0.003976  0.010513  0.006380  0.005306  0.005336  0.005452  0.006931  \n",
       "\n",
       "[18083 rows x 100 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.596580\n",
       "1        0.580670\n",
       "2        0.001657\n",
       "3        0.002715\n",
       "4        0.117356\n",
       "           ...   \n",
       "18078    0.003313\n",
       "18079    0.063521\n",
       "18080    0.002406\n",
       "18081    0.006934\n",
       "18082    0.008054\n",
       "Length: 18083, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthはdayと結合するつもりなので数字に直しました\n",
    "#今回のデータに12月は存在しません\n",
    "month_dict = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n",
    "month_int = [month_dict[train_df[\"month\"][i]] for i in range(len(train_df))]\n",
    "train_df[\"month\"] = month_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4\n",
       "1        2\n",
       "2        4\n",
       "3        1\n",
       "4        4\n",
       "        ..\n",
       "27123    2\n",
       "27124    5\n",
       "27125    5\n",
       "27126    8\n",
       "27127    5\n",
       "Name: month, Length: 27128, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        39\n",
       "1        51\n",
       "2        36\n",
       "3        63\n",
       "4        31\n",
       "         ..\n",
       "27123    42\n",
       "27124    34\n",
       "27125    36\n",
       "27126    33\n",
       "27127    29\n",
       "Name: age, Length: 27128, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['age'].hist(bins=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39,\n",
       " 51,\n",
       " 36,\n",
       " 63,\n",
       " 31,\n",
       " 29,\n",
       " 37,\n",
       " 32,\n",
       " 31,\n",
       " 32,\n",
       " 35,\n",
       " 34,\n",
       " 34,\n",
       " 31,\n",
       " 38,\n",
       " 71,\n",
       " 62,\n",
       " 66,\n",
       " 46,\n",
       " 29,\n",
       " 28,\n",
       " 67,\n",
       " 42,\n",
       " 33,\n",
       " 39,\n",
       " 57,\n",
       " 43,\n",
       " 46,\n",
       " 31,\n",
       " 36,\n",
       " 33,\n",
       " 52,\n",
       " 30,\n",
       " 41,\n",
       " 29,\n",
       " 56,\n",
       " 48,\n",
       " 30,\n",
       " 60,\n",
       " 32,\n",
       " 31,\n",
       " 29,\n",
       " 33,\n",
       " 31,\n",
       " 39,\n",
       " 23,\n",
       " 51,\n",
       " 39,\n",
       " 40,\n",
       " 69,\n",
       " 32,\n",
       " 30,\n",
       " 56,\n",
       " 32,\n",
       " 49,\n",
       " 45,\n",
       " 45,\n",
       " 55,\n",
       " 77,\n",
       " 30,\n",
       " 56,\n",
       " 54,\n",
       " 32,\n",
       " 43,\n",
       " 48,\n",
       " 50,\n",
       " 50,\n",
       " 35,\n",
       " 39,\n",
       " 36,\n",
       " 30,\n",
       " 43,\n",
       " 37,\n",
       " 53,\n",
       " 45,\n",
       " 30,\n",
       " 53,\n",
       " 45,\n",
       " 51,\n",
       " 35,\n",
       " 53,\n",
       " 33,\n",
       " 49,\n",
       " 36,\n",
       " 41,\n",
       " 36,\n",
       " 34,\n",
       " 37,\n",
       " 47,\n",
       " 54,\n",
       " 46,\n",
       " 36,\n",
       " 47,\n",
       " 34,\n",
       " 38,\n",
       " 35,\n",
       " 38,\n",
       " 42,\n",
       " 29,\n",
       " 56,\n",
       " 46,\n",
       " 34,\n",
       " 56,\n",
       " 60,\n",
       " 25,\n",
       " 29,\n",
       " 37,\n",
       " 33,\n",
       " 35,\n",
       " 46,\n",
       " 35,\n",
       " 34,\n",
       " 31,\n",
       " 39,\n",
       " 32,\n",
       " 37,\n",
       " 41,\n",
       " 39,\n",
       " 28,\n",
       " 34,\n",
       " 34,\n",
       " 32,\n",
       " 70,\n",
       " 30,\n",
       " 52,\n",
       " 51,\n",
       " 41,\n",
       " 39,\n",
       " 44,\n",
       " 58,\n",
       " 34,\n",
       " 38,\n",
       " 30,\n",
       " 48,\n",
       " 58,\n",
       " 34,\n",
       " 31,\n",
       " 37,\n",
       " 57,\n",
       " 28,\n",
       " 32,\n",
       " 32,\n",
       " 39,\n",
       " 41,\n",
       " 47,\n",
       " 28,\n",
       " 26,\n",
       " 60,\n",
       " 49,\n",
       " 32,\n",
       " 25,\n",
       " 25,\n",
       " 57,\n",
       " 50,\n",
       " 39,\n",
       " 34,\n",
       " 38,\n",
       " 30,\n",
       " 28,\n",
       " 31,\n",
       " 32,\n",
       " 28,\n",
       " 35,\n",
       " 48,\n",
       " 30,\n",
       " 46,\n",
       " 40,\n",
       " 37,\n",
       " 54,\n",
       " 26,\n",
       " 52,\n",
       " 67,\n",
       " 33,\n",
       " 42,\n",
       " 56,\n",
       " 35,\n",
       " 38,\n",
       " 33,\n",
       " 30,\n",
       " 35,\n",
       " 45,\n",
       " 28,\n",
       " 58,\n",
       " 58,\n",
       " 27,\n",
       " 39,\n",
       " 40,\n",
       " 37,\n",
       " 58,\n",
       " 60,\n",
       " 48,\n",
       " 45,\n",
       " 38,\n",
       " 29,\n",
       " 39,\n",
       " 43,\n",
       " 33,\n",
       " 32,\n",
       " 62,\n",
       " 49,\n",
       " 41,\n",
       " 56,\n",
       " 39,\n",
       " 50,\n",
       " 47,\n",
       " 48,\n",
       " 42,\n",
       " 59,\n",
       " 45,\n",
       " 46,\n",
       " 36,\n",
       " 53,\n",
       " 49,\n",
       " 33,\n",
       " 27,\n",
       " 34,\n",
       " 55,\n",
       " 34,\n",
       " 30,\n",
       " 30,\n",
       " 57,\n",
       " 37,\n",
       " 49,\n",
       " 42,\n",
       " 43,\n",
       " 40,\n",
       " 60,\n",
       " 70,\n",
       " 42,\n",
       " 27,\n",
       " 39,\n",
       " 43,\n",
       " 53,\n",
       " 37,\n",
       " 57,\n",
       " 43,\n",
       " 43,\n",
       " 49,\n",
       " 31,\n",
       " 28,\n",
       " 29,\n",
       " 36,\n",
       " 35,\n",
       " 37,\n",
       " 27,\n",
       " 30,\n",
       " 29,\n",
       " 49,\n",
       " 47,\n",
       " 41,\n",
       " 48,\n",
       " 51,\n",
       " 60,\n",
       " 57,\n",
       " 28,\n",
       " 61,\n",
       " 36,\n",
       " 22,\n",
       " 35,\n",
       " 34,\n",
       " 48,\n",
       " 33,\n",
       " 31,\n",
       " 26,\n",
       " 36,\n",
       " 39,\n",
       " 40,\n",
       " 32,\n",
       " 45,\n",
       " 40,\n",
       " 51,\n",
       " 44,\n",
       " 34,\n",
       " 45,\n",
       " 40,\n",
       " 51,\n",
       " 57,\n",
       " 30,\n",
       " 54,\n",
       " 28,\n",
       " 49,\n",
       " 39,\n",
       " 53,\n",
       " 35,\n",
       " 36,\n",
       " 40,\n",
       " 40,\n",
       " 32,\n",
       " 35,\n",
       " 35,\n",
       " 31,\n",
       " 44,\n",
       " 34,\n",
       " 49,\n",
       " 73,\n",
       " 38,\n",
       " 56,\n",
       " 27,\n",
       " 48,\n",
       " 36,\n",
       " 36,\n",
       " 60,\n",
       " 40,\n",
       " 57,\n",
       " 44,\n",
       " 50,\n",
       " 32,\n",
       " 44,\n",
       " 43,\n",
       " 33,\n",
       " 43,\n",
       " 24,\n",
       " 36,\n",
       " 49,\n",
       " 34,\n",
       " 37,\n",
       " 30,\n",
       " 58,\n",
       " 45,\n",
       " 38,\n",
       " 29,\n",
       " 36,\n",
       " 29,\n",
       " 25,\n",
       " 49,\n",
       " 33,\n",
       " 51,\n",
       " 35,\n",
       " 52,\n",
       " 42,\n",
       " 29,\n",
       " 43,\n",
       " 41,\n",
       " 47,\n",
       " 44,\n",
       " 37,\n",
       " 50,\n",
       " 42,\n",
       " 26,\n",
       " 39,\n",
       " 56,\n",
       " 22,\n",
       " 33,\n",
       " 30,\n",
       " 49,\n",
       " 51,\n",
       " 32,\n",
       " 25,\n",
       " 33,\n",
       " 45,\n",
       " 29,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 36,\n",
       " 25,\n",
       " 31,\n",
       " 29,\n",
       " 29,\n",
       " 51,\n",
       " 35,\n",
       " 32,\n",
       " 37,\n",
       " 45,\n",
       " 46,\n",
       " 64,\n",
       " 60,\n",
       " 48,\n",
       " 28,\n",
       " 30,\n",
       " 39,\n",
       " 51,\n",
       " 50,\n",
       " 26,\n",
       " 36,\n",
       " 30,\n",
       " 59,\n",
       " 39,\n",
       " 31,\n",
       " 47,\n",
       " 52,\n",
       " 48,\n",
       " 47,\n",
       " 65,\n",
       " 43,\n",
       " 32,\n",
       " 45,\n",
       " 54,\n",
       " 33,\n",
       " 30,\n",
       " 50,\n",
       " 36,\n",
       " 51,\n",
       " 30,\n",
       " 40,\n",
       " 71,\n",
       " 58,\n",
       " 26,\n",
       " 61,\n",
       " 36,\n",
       " 30,\n",
       " 34,\n",
       " 49,\n",
       " 51,\n",
       " 51,\n",
       " 59,\n",
       " 43,\n",
       " 43,\n",
       " 40,\n",
       " 34,\n",
       " 50,\n",
       " 32,\n",
       " 41,\n",
       " 56,\n",
       " 36,\n",
       " 36,\n",
       " 34,\n",
       " 57,\n",
       " 50,\n",
       " 46,\n",
       " 60,\n",
       " 30,\n",
       " 31,\n",
       " 51,\n",
       " 44,\n",
       " 27,\n",
       " 27,\n",
       " 54,\n",
       " 56,\n",
       " 44,\n",
       " 30,\n",
       " 37,\n",
       " 32,\n",
       " 44,\n",
       " 34,\n",
       " 32,\n",
       " 57,\n",
       " 26,\n",
       " 45,\n",
       " 32,\n",
       " 51,\n",
       " 54,\n",
       " 48,\n",
       " 36,\n",
       " 36,\n",
       " 26,\n",
       " 34,\n",
       " 25,\n",
       " 54,\n",
       " 34,\n",
       " 26,\n",
       " 29,\n",
       " 49,\n",
       " 37,\n",
       " 41,\n",
       " 50,\n",
       " 36,\n",
       " 29,\n",
       " 58,\n",
       " 32,\n",
       " 38,\n",
       " 36,\n",
       " 55,\n",
       " 30,\n",
       " 56,\n",
       " 26,\n",
       " 31,\n",
       " 50,\n",
       " 35,\n",
       " 41,\n",
       " 35,\n",
       " 33,\n",
       " 42,\n",
       " 32,\n",
       " 28,\n",
       " 55,\n",
       " 39,\n",
       " 31,\n",
       " 35,\n",
       " 53,\n",
       " 60,\n",
       " 32,\n",
       " 19,\n",
       " 36,\n",
       " 45,\n",
       " 25,\n",
       " 41,\n",
       " 40,\n",
       " 32,\n",
       " 32,\n",
       " 52,\n",
       " 26,\n",
       " 34,\n",
       " 41,\n",
       " 55,\n",
       " 45,\n",
       " 48,\n",
       " 60,\n",
       " 45,\n",
       " 32,\n",
       " 56,\n",
       " 55,\n",
       " 39,\n",
       " 55,\n",
       " 34,\n",
       " 40,\n",
       " 41,\n",
       " 56,\n",
       " 33,\n",
       " 42,\n",
       " 39,\n",
       " 43,\n",
       " 28,\n",
       " 37,\n",
       " 30,\n",
       " 44,\n",
       " 50,\n",
       " 30,\n",
       " 36,\n",
       " 51,\n",
       " 37,\n",
       " 32,\n",
       " 79,\n",
       " 56,\n",
       " 37,\n",
       " 37,\n",
       " 47,\n",
       " 41,\n",
       " 48,\n",
       " 58,\n",
       " 47,\n",
       " 41,\n",
       " 51,\n",
       " 38,\n",
       " 36,\n",
       " 45,\n",
       " 40,\n",
       " 52,\n",
       " 37,\n",
       " 36,\n",
       " 56,\n",
       " 46,\n",
       " 27,\n",
       " 64,\n",
       " 41,\n",
       " 34,\n",
       " 36,\n",
       " 44,\n",
       " 38,\n",
       " 57,\n",
       " 72,\n",
       " 34,\n",
       " 32,\n",
       " 39,\n",
       " 69,\n",
       " 49,\n",
       " 54,\n",
       " 48,\n",
       " 35,\n",
       " 24,\n",
       " 32,\n",
       " 43,\n",
       " 54,\n",
       " 41,\n",
       " 48,\n",
       " 40,\n",
       " 44,\n",
       " 47,\n",
       " 55,\n",
       " 29,\n",
       " 37,\n",
       " 58,\n",
       " 41,\n",
       " 49,\n",
       " 38,\n",
       " 46,\n",
       " 55,\n",
       " 58,\n",
       " 33,\n",
       " 43,\n",
       " 44,\n",
       " 46,\n",
       " 36,\n",
       " 49,\n",
       " 46,\n",
       " 31,\n",
       " 27,\n",
       " 30,\n",
       " 34,\n",
       " 38,\n",
       " 49,\n",
       " 37,\n",
       " 47,\n",
       " 26,\n",
       " 64,\n",
       " 38,\n",
       " 58,\n",
       " 36,\n",
       " 33,\n",
       " 47,\n",
       " 54,\n",
       " 31,\n",
       " 39,\n",
       " 59,\n",
       " 43,\n",
       " 30,\n",
       " 34,\n",
       " 51,\n",
       " 37,\n",
       " 44,\n",
       " 26,\n",
       " 32,\n",
       " 33,\n",
       " 40,\n",
       " 31,\n",
       " 34,\n",
       " 37,\n",
       " 54,\n",
       " 36,\n",
       " 26,\n",
       " 26,\n",
       " 31,\n",
       " 60,\n",
       " 41,\n",
       " 30,\n",
       " 54,\n",
       " 34,\n",
       " 29,\n",
       " 32,\n",
       " 48,\n",
       " 50,\n",
       " 31,\n",
       " 32,\n",
       " 73,\n",
       " 57,\n",
       " 34,\n",
       " 49,\n",
       " 37,\n",
       " 32,\n",
       " 27,\n",
       " 78,\n",
       " 28,\n",
       " 49,\n",
       " 31,\n",
       " 49,\n",
       " 25,\n",
       " 41,\n",
       " 41,\n",
       " 31,\n",
       " 43,\n",
       " 43,\n",
       " 42,\n",
       " 32,\n",
       " 38,\n",
       " 31,\n",
       " 27,\n",
       " 28,\n",
       " 59,\n",
       " 50,\n",
       " 30,\n",
       " 56,\n",
       " 26,\n",
       " 57,\n",
       " 32,\n",
       " 49,\n",
       " 40,\n",
       " 55,\n",
       " 54,\n",
       " 29,\n",
       " 42,\n",
       " 26,\n",
       " 47,\n",
       " 38,\n",
       " 32,\n",
       " 36,\n",
       " 32,\n",
       " 36,\n",
       " 34,\n",
       " 45,\n",
       " 29,\n",
       " 25,\n",
       " 40,\n",
       " 27,\n",
       " 51,\n",
       " 52,\n",
       " 59,\n",
       " 58,\n",
       " 51,\n",
       " 44,\n",
       " 36,\n",
       " 66,\n",
       " 33,\n",
       " 29,\n",
       " 45,\n",
       " 34,\n",
       " 32,\n",
       " 58,\n",
       " 33,\n",
       " 54,\n",
       " 34,\n",
       " 37,\n",
       " 52,\n",
       " 26,\n",
       " 34,\n",
       " 62,\n",
       " 30,\n",
       " 33,\n",
       " 33,\n",
       " 52,\n",
       " 57,\n",
       " 50,\n",
       " 27,\n",
       " 30,\n",
       " 52,\n",
       " 35,\n",
       " 25,\n",
       " 42,\n",
       " 43,\n",
       " 31,\n",
       " 36,\n",
       " 27,\n",
       " 53,\n",
       " 39,\n",
       " 22,\n",
       " 53,\n",
       " 26,\n",
       " 54,\n",
       " 64,\n",
       " 38,\n",
       " 24,\n",
       " 50,\n",
       " 46,\n",
       " 39,\n",
       " 36,\n",
       " 40,\n",
       " 54,\n",
       " 45,\n",
       " 25,\n",
       " 46,\n",
       " 43,\n",
       " 32,\n",
       " 45,\n",
       " 49,\n",
       " 25,\n",
       " 50,\n",
       " 31,\n",
       " 30,\n",
       " 46,\n",
       " 39,\n",
       " 41,\n",
       " 51,\n",
       " 36,\n",
       " 33,\n",
       " 35,\n",
       " 29,\n",
       " 33,\n",
       " 34,\n",
       " 40,\n",
       " 45,\n",
       " 38,\n",
       " 25,\n",
       " 41,\n",
       " 48,\n",
       " 47,\n",
       " 32,\n",
       " 46,\n",
       " 38,\n",
       " 52,\n",
       " 34,\n",
       " 26,\n",
       " 47,\n",
       " 37,\n",
       " 36,\n",
       " 46,\n",
       " 53,\n",
       " 51,\n",
       " 41,\n",
       " 41,\n",
       " 28,\n",
       " 38,\n",
       " 40,\n",
       " 55,\n",
       " 50,\n",
       " 44,\n",
       " 39,\n",
       " 44,\n",
       " 56,\n",
       " 53,\n",
       " 51,\n",
       " 43,\n",
       " 30,\n",
       " 43,\n",
       " 52,\n",
       " 39,\n",
       " 50,\n",
       " 35,\n",
       " 47,\n",
       " 38,\n",
       " 49,\n",
       " 43,\n",
       " 32,\n",
       " 29,\n",
       " 33,\n",
       " 37,\n",
       " 44,\n",
       " 37,\n",
       " 59,\n",
       " 36,\n",
       " 59,\n",
       " 33,\n",
       " 34,\n",
       " 55,\n",
       " 40,\n",
       " 33,\n",
       " 39,\n",
       " 34,\n",
       " 38,\n",
       " 36,\n",
       " 39,\n",
       " 32,\n",
       " 39,\n",
       " 26,\n",
       " 31,\n",
       " 30,\n",
       " 42,\n",
       " 28,\n",
       " 59,\n",
       " 34,\n",
       " 27,\n",
       " 38,\n",
       " 31,\n",
       " 41,\n",
       " 28,\n",
       " 35,\n",
       " 34,\n",
       " 37,\n",
       " 46,\n",
       " 49,\n",
       " 28,\n",
       " 37,\n",
       " 56,\n",
       " 33,\n",
       " 59,\n",
       " 39,\n",
       " 48,\n",
       " 27,\n",
       " 35,\n",
       " 29,\n",
       " 33,\n",
       " 50,\n",
       " 33,\n",
       " 30,\n",
       " 47,\n",
       " 47,\n",
       " 49,\n",
       " 29,\n",
       " 47,\n",
       " 32,\n",
       " 40,\n",
       " 35,\n",
       " 60,\n",
       " 45,\n",
       " 35,\n",
       " 30,\n",
       " 31,\n",
       " 63,\n",
       " 49,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 54,\n",
       " 39,\n",
       " 34,\n",
       " 46,\n",
       " 31,\n",
       " 45,\n",
       " 45,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 33,\n",
       " 38,\n",
       " 46,\n",
       " 34,\n",
       " 25,\n",
       " 34,\n",
       " 30,\n",
       " 31,\n",
       " 39,\n",
       " 47,\n",
       " 41,\n",
       " 31,\n",
       " 51,\n",
       " 31,\n",
       " 39,\n",
       " 28,\n",
       " 48,\n",
       " 53,\n",
       " 38,\n",
       " 52,\n",
       " 34,\n",
       " 38,\n",
       " 33,\n",
       " 59,\n",
       " 41,\n",
       " 40,\n",
       " 55,\n",
       " 39,\n",
       " 32,\n",
       " 52,\n",
       " 34,\n",
       " 35,\n",
       " 49,\n",
       " 28,\n",
       " 45,\n",
       " 46,\n",
       " 34,\n",
       " 46,\n",
       " 57,\n",
       " 41,\n",
       " 24,\n",
       " 35,\n",
       " 29,\n",
       " 23,\n",
       " 58,\n",
       " 42,\n",
       " 50,\n",
       " 32,\n",
       " 31,\n",
       " 45,\n",
       " 58,\n",
       " 42,\n",
       " 47,\n",
       " 37,\n",
       " 57,\n",
       " 30,\n",
       " 47,\n",
       " 42,\n",
       " 33,\n",
       " 34,\n",
       " 37,\n",
       " 28,\n",
       " 27,\n",
       " 34,\n",
       " 37,\n",
       " 49,\n",
       " 47,\n",
       " 36,\n",
       " 58,\n",
       " 54,\n",
       " 31,\n",
       " 42,\n",
       " 34,\n",
       " 51,\n",
       " 50,\n",
       " 30,\n",
       " 74,\n",
       " 33,\n",
       " 56,\n",
       " 43,\n",
       " 48,\n",
       " 31,\n",
       " 58,\n",
       " 37,\n",
       " 47,\n",
       " 45,\n",
       " 54,\n",
       " 28,\n",
       " 35,\n",
       " 27,\n",
       " 56,\n",
       " 68,\n",
       " 48,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 59,\n",
       " 29,\n",
       " 29,\n",
       " 42,\n",
       " 52,\n",
       " 37,\n",
       " 39,\n",
       " 54,\n",
       " 25,\n",
       " 58,\n",
       " 59,\n",
       " 31,\n",
       " 42,\n",
       " 26,\n",
       " 54,\n",
       " 42,\n",
       " 29,\n",
       " 26,\n",
       " 37,\n",
       " 36,\n",
       " 49,\n",
       " 49,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_df[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#年齢\n",
    "#age\n",
    "#60歳以上の扱いに悩んだので丸めました\n",
    "#～59歳までは十代ごとに、60歳ちょうど、60歳より上に変換\n",
    "age_list = list(train_df[\"age\"])\n",
    "\n",
    "new_age_list = []\n",
    "\n",
    "for i in range(len(age_list)):\n",
    "    if age_list[i] == 60:\n",
    "        new_age_list.append(60)\n",
    "    elif age_list[i] > 60:\n",
    "        new_age_list.append(70)\n",
    "    else:\n",
    "        new_age_list.append(int(age_list[i]/10)*10)\n",
    "\n",
    "train_df[\"age_round\"] = new_age_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        30\n",
       "1        50\n",
       "2        30\n",
       "3        70\n",
       "4        30\n",
       "         ..\n",
       "27123    40\n",
       "27124    30\n",
       "27125    30\n",
       "27126    30\n",
       "27127    20\n",
       "Name: age_round, Length: 27128, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"age_round\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_round(x):\n",
    "    if x == 60:\n",
    "        return 60\n",
    "    elif x > 60:\n",
    "        return 70\n",
    "    else:\n",
    "        return int(x/10)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        30\n",
       "1        50\n",
       "2        30\n",
       "3        70\n",
       "4        30\n",
       "         ..\n",
       "27123    40\n",
       "27124    30\n",
       "27125    30\n",
       "27126    30\n",
       "27127    20\n",
       "Name: age_round2, Length: 27128, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['age_round2'] = train_df['age'].apply(age_round)\n",
    "train_df['age_round2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>y</th>\n",
       "      <th>age_round</th>\n",
       "      <th>age_round2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "      <td>27128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13564.500000</td>\n",
       "      <td>40.951010</td>\n",
       "      <td>1355.800870</td>\n",
       "      <td>15.806215</td>\n",
       "      <td>6.132262</td>\n",
       "      <td>260.711295</td>\n",
       "      <td>2.751769</td>\n",
       "      <td>40.528052</td>\n",
       "      <td>0.579733</td>\n",
       "      <td>0.117001</td>\n",
       "      <td>36.584710</td>\n",
       "      <td>36.584710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7831.323388</td>\n",
       "      <td>10.608542</td>\n",
       "      <td>3003.305272</td>\n",
       "      <td>8.337904</td>\n",
       "      <td>2.402256</td>\n",
       "      <td>260.091727</td>\n",
       "      <td>3.126594</td>\n",
       "      <td>100.382462</td>\n",
       "      <td>2.503653</td>\n",
       "      <td>0.321427</td>\n",
       "      <td>11.076654</td>\n",
       "      <td>11.076654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>-6847.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6782.750000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13564.500000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20346.250000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27128.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>102127.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4918.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id           age        balance           day         month  \\\n",
       "count  27128.000000  27128.000000   27128.000000  27128.000000  27128.000000   \n",
       "mean   13564.500000     40.951010    1355.800870     15.806215      6.132262   \n",
       "std     7831.323388     10.608542    3003.305272      8.337904      2.402256   \n",
       "min        1.000000     18.000000   -6847.000000      1.000000      1.000000   \n",
       "25%     6782.750000     33.000000      72.000000      8.000000      5.000000   \n",
       "50%    13564.500000     39.000000     449.000000     16.000000      6.000000   \n",
       "75%    20346.250000     48.000000    1428.000000     21.000000      8.000000   \n",
       "max    27128.000000     95.000000  102127.000000     31.000000     12.000000   \n",
       "\n",
       "           duration      campaign         pdays      previous             y  \\\n",
       "count  27128.000000  27128.000000  27128.000000  27128.000000  27128.000000   \n",
       "mean     260.711295      2.751769     40.528052      0.579733      0.117001   \n",
       "std      260.091727      3.126594    100.382462      2.503653      0.321427   \n",
       "min        0.000000      1.000000     -1.000000      0.000000      0.000000   \n",
       "25%      104.000000      1.000000     -1.000000      0.000000      0.000000   \n",
       "50%      182.000000      2.000000     -1.000000      0.000000      0.000000   \n",
       "75%      323.000000      3.000000     -1.000000      0.000000      0.000000   \n",
       "max     4918.000000     63.000000    871.000000    275.000000      1.000000   \n",
       "\n",
       "          age_round    age_round2  \n",
       "count  27128.000000  27128.000000  \n",
       "mean      36.584710     36.584710  \n",
       "std       11.076654     11.076654  \n",
       "min       10.000000     10.000000  \n",
       "25%       30.000000     30.000000  \n",
       "50%       30.000000     30.000000  \n",
       "75%       40.000000     40.000000  \n",
       "max       70.000000     70.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAS30lEQVR4nO3db4xd9X3n8fendkMcXAKUZGRh2iGqlRbwhoQRdZu2GkJ2cRsU86BIjkgxFStLiHbJylVr+iRqJUv0AW2DWpCskGKabLwOTYsVRDbI6Wj/iMDaSVbGOBZW8IKDazdtQnCa0gz99sH9Wbkdj419z/y5136/pKt77vec37m/LzPjD+ec+ydVhSRJP7bYE5AkDQcDQZIEGAiSpMZAkCQBBoIkqVm62BMY1GWXXVbj4+MDjf3+97/PhRdeOLcTWiT2MnzOlT7AXoZVl1727Nnz7ap6x2zrRjYQxsfH2b1790Bjp6ammJycnNsJLRJ7GT7nSh9gL8OqSy9J/v+p1nnKSJIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgSM8DuVNZzGNz8x8NhNq6e5o8P4QRy670ML+nzSMPMIQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpedNASPKpJMeSPNdXuzTJU0leaPeX9K27N8nBJAeS3NRXvy7J3rbugSRp9QuS/PdWfybJ+Bz3KEk6A2dyhPAIsHZGbTOwq6pWAbvaY5JcBawHrm5jHkyypI15CNgIrGq3E/u8E/hOVf0M8CfAHw3ajCRpcG8aCFX1P4F/nFFeB2xry9uAW/rq26vq9ap6ETgIXJ9kBXBRVT1dVQU8OmPMiX09Btx44uhBkrRwBv0+hLGqOgJQVUeSvLPVLwe+0rfd4Vb7YVueWT8x5uW2r+kkrwI/CXx75pMm2UjvKIOxsTGmpqYGmvzx48cHHjtshq2XTaunBx47tqzb+EHMx3+7YfuZdGEvw2m+epnrL8iZ7f/s6zT10405uVi1FdgKMDExUZOTkwNMsfePwKBjh82w9dLlC242rZ7m/r0L+51Nh26bnPN9DtvPpAt7GU7z1cugrzI62k4D0e6Ptfph4Iq+7VYCr7T6ylnq/25MkqXA2zn5FJUkaZ4NGgg7gQ1teQPweF99fXvl0JX0Lh4/204vvZZkTbs+cPuMMSf29evAl9t1BknSAnrT4/MknwUmgcuSHAY+DtwH7EhyJ/AScCtAVe1LsgN4HpgG7q6qN9qu7qL3iqVlwJPtBvAw8JdJDtI7Mlg/J51Jks7KmwZCVX3kFKtuPMX2W4Ats9R3A9fMUv9nWqBIkhaP71SWJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEtAxEJL81yT7kjyX5LNJ3prk0iRPJXmh3V/St/29SQ4mOZDkpr76dUn2tnUPJEmXeUmSzt7AgZDkcuC/ABNVdQ2wBFgPbAZ2VdUqYFd7TJKr2vqrgbXAg0mWtN09BGwEVrXb2kHnJUkaTNdTRkuBZUmWAm8DXgHWAdva+m3ALW15HbC9ql6vqheBg8D1SVYAF1XV01VVwKN9YyRJCyS9f4MHHJzcA2wBfgB8qapuS/Ldqrq4b5vvVNUlSf4M+EpVfbrVHwaeBA4B91XVB1v9l4Hfq6qbZ3m+jfSOJBgbG7tu+/btA837+PHjLF++fKCxw2bYetn7rVcHHju2DI7+YA4ncwZWX/72Od/nsP1MurCX4dSllxtuuGFPVU3Mtm7poBNq1wbWAVcC3wU+l+SjpxsyS61OUz+5WLUV2AowMTFRk5OTZzHjH5mammLQscNm2Hq5Y/MTA4/dtHqa+/cO/Cs5kEO3Tc75PoftZ9KFvQyn+eqlyymjDwIvVtXfV9UPgc8DvwgcbaeBaPfH2vaHgSv6xq+kd4rpcFueWZckLaAugfASsCbJ29qrgm4E9gM7gQ1tmw3A4215J7A+yQVJrqR38fjZqjoCvJZkTdvP7X1jJEkLZODj86p6JsljwFeBaeBr9E7nLAd2JLmTXmjc2rbfl2QH8Hzb/u6qeqPt7i7gEWAZvesKTw46L0nSYDqdsK2qjwMfn1F+nd7Rwmzbb6F3EXpmfTdwTZe5SJK68Z3KkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAjp+2qk06sY7fMPbqWxaPd3pm+NO5dB9H5rzfUr9PEKQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEdAyEJBcneSzJN5LsT/ILSS5N8lSSF9r9JX3b35vkYJIDSW7qq1+XZG9b90CSdJmXJOnsdT1C+ATwxar6WeA9wH5gM7CrqlYBu9pjklwFrAeuBtYCDyZZ0vbzELARWNVuazvOS5J0lgYOhCQXAb8CPAxQVf9SVd8F1gHb2mbbgFva8jpge1W9XlUvAgeB65OsAC6qqqerqoBH+8ZIkhZIev8GDzAwuRbYCjxP7+hgD3AP8K2qurhvu+9U1SVJ/gz4SlV9utUfBp4EDgH3VdUHW/2Xgd+rqptnec6N9I4kGBsbu2779u0Dzf348eMsX758oLHDZth62futVwceO7YMjv5gDiezSOarj9WXv33ud/omhu33qwt76bnhhhv2VNXEbOu6fKfyUuB9wG9X1TNJPkE7PXQKs10XqNPUTy5WbaUXQkxMTNTk5ORZTfiEqakpBh07bIatly7fJbxp9TT37x39r/merz4O3TY55/t8M8P2+9WFvby5LtcQDgOHq+qZ9vgxegFxtJ0Got0f69v+ir7xK4FXWn3lLHVJ0gIaOBCq6u+Al5O8u5VupHf6aCewodU2AI+35Z3A+iQXJLmS3sXjZ6vqCPBakjXt1UW3942RJC2Qrse1vw18JslbgG8Cv0kvZHYkuRN4CbgVoKr2JdlBLzSmgbur6o22n7uAR4Bl9K4rPNlxXpKks9QpEKrq68BsFyduPMX2W4Ats9R3A9d0mYskqRvfqSxJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJKA7h9uJ2mBjHf4rolBbVo9PfB3XBy670NzPBvNN48QJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJwBwEQpIlSb6W5Avt8aVJnkryQru/pG/be5McTHIgyU199euS7G3rHkiSrvOSJJ2duThCuAfY3/d4M7CrqlYBu9pjklwFrAeuBtYCDyZZ0sY8BGwEVrXb2jmYlyTpLHQKhCQrgQ8Bn+wrrwO2teVtwC199e1V9XpVvQgcBK5PsgK4qKqerqoCHu0bI0laIEs7jv9T4HeBn+irjVXVEYCqOpLkna1+OfCVvu0Ot9oP2/LM+kmSbKR3JMHY2BhTU1MDTfr48eMDjx02w9bLptXTA48dW9Zt/LA4V/qAbr0M0+8lDN/fShfz1cvAgZDkZuBYVe1JMnkmQ2ap1WnqJxertgJbASYmJmpy8kye9mRTU1MMOnbYDFsvd2x+YuCxm1ZPc//erv+PsvjOlT6gWy+Hbpuc28l0NGx/K13MVy9dfmvfD3w4ya8BbwUuSvJp4GiSFe3oYAVwrG1/GLiib/xK4JVWXzlLXZK0gAa+hlBV91bVyqoap3ex+MtV9VFgJ7ChbbYBeLwt7wTWJ7kgyZX0Lh4/204vvZZkTXt10e19YyRJC2Q+jmvvA3YkuRN4CbgVoKr2JdkBPA9MA3dX1RttzF3AI8Ay4Ml2kyQtoDkJhKqaAqba8j8AN55iuy3Allnqu4Fr5mIukqTB+E5lSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBMzPh9tpjo2f5jsGNq2e7vQdBJJ0gkcIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkoEMgJLkiyd8m2Z9kX5J7Wv3SJE8leaHdX9I35t4kB5McSHJTX/26JHvbugeSpFtbkqSz1eUIYRrYVFU/B6wB7k5yFbAZ2FVVq4Bd7TFt3XrgamAt8GCSJW1fDwEbgVXttrbDvCRJAxg4EKrqSFV9tS2/BuwHLgfWAdvaZtuAW9ryOmB7Vb1eVS8CB4Hrk6wALqqqp6uqgEf7xkiSFsicXENIMg68F3gGGKuqI9ALDeCdbbPLgZf7hh1utcvb8sy6JGkBLe26gyTLgb8CPlZV3zvN6f/ZVtRp6rM910Z6p5YYGxtjamrqrOcLcPz48YHHLoZNq6dPuW5s2enXj5JzpZdzpQ/o1suw/Y2N2t/96cxXL50CIcmP0wuDz1TV51v5aJIVVXWknQ461uqHgSv6hq8EXmn1lbPUT1JVW4GtABMTEzU5OTnQvKemphh07GK4Y/MTp1y3afU09+/tnOtD4Vzp5VzpA7r1cui2ybmdTEej9nd/OvPVS5dXGQV4GNhfVX/ct2onsKEtbwAe76uvT3JBkivpXTx+tp1Wei3JmrbP2/vGSJIWSJf/jXk/8BvA3iRfb7XfB+4DdiS5E3gJuBWgqvYl2QE8T+8VSndX1Rtt3F3AI8Ay4Ml2kyQtoIEDoar+N7Of/we48RRjtgBbZqnvBq4ZdC6SpO58p7IkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQI6fKeyJJ0rxjc/sdhTOCuPrL1wXvbrEYIkCTAQJEmNgSBJAgwESVJjIEiSAANBktT4slNJ82LYXsq5afU0dwzZnIaNRwiSJMBAkCQ1BoIkCRiiQEiyNsmBJAeTbF7s+UjS+WYoAiHJEuDPgV8FrgI+kuSqxZ2VJJ1fhuVVRtcDB6vqmwBJtgPrgOfn48n2futVX20gSTOkqhZ7DiT5dWBtVf3n9vg3gJ+vqt+asd1GYGN7+G7gwIBPeRnw7QHHDht7GT7nSh9gL8OqSy8/XVXvmG3FsBwhZJbaSUlVVVuBrZ2fLNldVRNd9zMM7GX4nCt9gL0Mq/nqZSiuIQCHgSv6Hq8EXlmkuUjSeWlYAuH/AquSXJnkLcB6YOciz0mSzitDccqoqqaT/BbwP4AlwKeqat88PmXn005DxF6Gz7nSB9jLsJqXXobiorIkafENyykjSdIiMxAkScB5EAhJPpXkWJLn+mqXJnkqyQvt/pLFnOOZSHJFkr9Nsj/JviT3tPoo9vLWJM8m+X+tlz9o9ZHr5YQkS5J8LckX2uOR7CXJoSR7k3w9ye5WG7leklyc5LEk32h/M78won28u/0sTty+l+Rj89XLOR8IwCPA2hm1zcCuqloF7GqPh900sKmqfg5YA9zdPt5jFHt5HfhAVb0HuBZYm2QNo9nLCfcA+/sej3IvN1TVtX2vcx/FXj4BfLGqfhZ4D72fzcj1UVUH2s/iWuA64J+Av2a+eqmqc/4GjAPP9T0+AKxoyyuAA4s9xwF6ehz4j6PeC/A24KvAz49qL/TeN7ML+ADwhVYb1V4OAZfNqI1UL8BFwIu0F82Mah+z9PWfgP8zn72cD0cIsxmrqiMA7f6dizyfs5JkHHgv8Awj2ks7xfJ14BjwVFWNbC/AnwK/C/xrX21UeyngS0n2tI+KgdHr5V3A3wN/0U7jfTLJhYxeHzOtBz7bluell/M1EEZWkuXAXwEfq6rvLfZ8BlVVb1TvMHglcH2SaxZ5SgNJcjNwrKr2LPZc5sj7q+p99D55+O4kv7LYExrAUuB9wENV9V7g+4zA6aHTaW/Y/TDwufl8nvM1EI4mWQHQ7o8t8nzOSJIfpxcGn6mqz7fySPZyQlV9F5iid51nFHt5P/DhJIeA7cAHknya0eyFqnql3R+jd676ekavl8PA4XbUCfAYvYAYtT76/Srw1ao62h7PSy/nayDsBDa05Q30zscPtSQBHgb2V9Uf960axV7ekeTitrwM+CDwDUawl6q6t6pWVtU4vUP6L1fVRxnBXpJcmOQnTizTO2f9HCPWS1X9HfBykne30o30Pkp/pPqY4SP86HQRzFMv5/w7lZN8Fpik93GxR4GPA38D7AB+CngJuLWq/nGRpnhGkvwS8L+AvfzoXPXv07uOMGq9/AdgG72PKfkxYEdV/WGSn2TEeumXZBL4naq6eRR7SfIuekcF0Dvt8t+qasuI9nIt8EngLcA3gd+k/a4xQn0AJHkb8DLwrqp6tdXm5WdyzgeCJOnMnK+njCRJMxgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElS82+p90HnDkCL6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['age_round2'].hist(bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27128 entries, 0 to 27127\n",
      "Data columns (total 20 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          27128 non-null  int64 \n",
      " 1   age         27128 non-null  int64 \n",
      " 2   job         27128 non-null  object\n",
      " 3   marital     27128 non-null  object\n",
      " 4   education   27128 non-null  object\n",
      " 5   default     27128 non-null  object\n",
      " 6   balance     27128 non-null  int64 \n",
      " 7   housing     27128 non-null  object\n",
      " 8   loan        27128 non-null  object\n",
      " 9   contact     27128 non-null  object\n",
      " 10  day         27128 non-null  int64 \n",
      " 11  month       27128 non-null  int64 \n",
      " 12  duration    27128 non-null  int64 \n",
      " 13  campaign    27128 non-null  int64 \n",
      " 14  pdays       27128 non-null  int64 \n",
      " 15  previous    27128 non-null  int64 \n",
      " 16  poutcome    27128 non-null  object\n",
      " 17  y           27128 non-null  int64 \n",
      " 18  age_round   27128 non-null  int64 \n",
      " 19  age_round2  27128 non-null  int64 \n",
      "dtypes: int64(12), object(8)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col_list = [\"age\",\"balance\",\"duration\",\"campaign\",\"pdays\",\"previous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col_list  = [categorical_feature for categorical_feature in train_df.columns if categorical_feature not in numeric_col_list]\n",
    "categorical_col_list.remove(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'day',\n",
       " 'month',\n",
       " 'poutcome',\n",
       " 'y',\n",
       " 'age_round',\n",
       " 'age_round2']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'age',\n",
       " 'balance',\n",
       " 'day',\n",
       " 'month',\n",
       " 'duration',\n",
       " 'campaign',\n",
       " 'pdays',\n",
       " 'previous',\n",
       " 'y',\n",
       " 'age_round',\n",
       " 'age_round2']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select_dtypes(exclude='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： age\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23954.0</td>\n",
       "      <td>40.870335</td>\n",
       "      <td>10.175948</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3174.0</td>\n",
       "      <td>41.559861</td>\n",
       "      <td>13.417584</td>\n",
       "      <td>18.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count       mean        std   min   25%   50%   75%   max\n",
       "y                                                             \n",
       "0  23954.0  40.870335  10.175948  18.0  33.0  39.0  48.0  95.0\n",
       "1   3174.0  41.559861  13.417584  18.0  31.0  38.0  50.0  93.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： balance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23954.0</td>\n",
       "      <td>1305.702805</td>\n",
       "      <td>2996.408734</td>\n",
       "      <td>-6847.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>1344.00</td>\n",
       "      <td>102127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3174.0</td>\n",
       "      <td>1733.888154</td>\n",
       "      <td>3028.712596</td>\n",
       "      <td>-3058.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>718.5</td>\n",
       "      <td>2148.25</td>\n",
       "      <td>52587.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count         mean          std     min    25%    50%      75%       max\n",
       "y                                                                            \n",
       "0  23954.0  1305.702805  2996.408734 -6847.0   58.0  417.0  1344.00  102127.0\n",
       "1   3174.0  1733.888154  3028.712596 -3058.0  215.0  718.5  2148.25   52587.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： duration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23954.0</td>\n",
       "      <td>222.709861</td>\n",
       "      <td>207.103447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.00</td>\n",
       "      <td>165.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>4918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3174.0</td>\n",
       "      <td>547.505986</td>\n",
       "      <td>401.711459</td>\n",
       "      <td>8.0</td>\n",
       "      <td>247.25</td>\n",
       "      <td>434.5</td>\n",
       "      <td>731.0</td>\n",
       "      <td>3102.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count        mean         std  min     25%    50%    75%     max\n",
       "y                                                                    \n",
       "0  23954.0  222.709861  207.103447  0.0   96.00  165.0  281.0  4918.0\n",
       "1   3174.0  547.505986  401.711459  8.0  247.25  434.5  731.0  3102.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： campaign\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23954.0</td>\n",
       "      <td>2.838399</td>\n",
       "      <td>3.251450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3174.0</td>\n",
       "      <td>2.097984</td>\n",
       "      <td>1.811868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count      mean       std  min  25%  50%  75%   max\n",
       "y                                                       \n",
       "0  23954.0  2.838399  3.251450  1.0  1.0  2.0  3.0  63.0\n",
       "1   3174.0  2.097984  1.811868  1.0  1.0  2.0  2.0  24.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： pdays\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23954.0</td>\n",
       "      <td>36.840110</td>\n",
       "      <td>97.375842</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>871.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3174.0</td>\n",
       "      <td>68.360744</td>\n",
       "      <td>117.007492</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>98.75</td>\n",
       "      <td>805.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count       mean         std  min  25%  50%    75%    max\n",
       "y                                                             \n",
       "0  23954.0  36.840110   97.375842 -1.0 -1.0 -1.0  -1.00  871.0\n",
       "1   3174.0  68.360744  117.007492 -1.0 -1.0 -1.0  98.75  805.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： previous\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23954.0</td>\n",
       "      <td>0.50334</td>\n",
       "      <td>2.494818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3174.0</td>\n",
       "      <td>1.15627</td>\n",
       "      <td>2.495410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count     mean       std  min  25%  50%  75%    max\n",
       "y                                                       \n",
       "0  23954.0  0.50334  2.494818  0.0  0.0  0.0  0.0  275.0\n",
       "1   3174.0  1.15627  2.495410  0.0  0.0  0.0  1.0   55.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#numericについては目的変数でグルーピングして統計量を算出\n",
    "for target_col in numeric_col_list:\n",
    "    print(\"\\ntarget_col：\",target_col)\n",
    "    display(train_df.groupby(\"y\")[target_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： job\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  job          \n",
       "0  admin.           2700\n",
       "   blue-collar      5456\n",
       "   entrepreneur      834\n",
       "   housemaid         697\n",
       "   management       4853\n",
       "   retired          1080\n",
       "   self-employed     828\n",
       "   services         2282\n",
       "   student           391\n",
       "   technician       4013\n",
       "   unemployed        660\n",
       "   unknown           160\n",
       "1  admin.            385\n",
       "   blue-collar       430\n",
       "   entrepreneur       80\n",
       "   housemaid          68\n",
       "   management        767\n",
       "   retired           311\n",
       "   self-employed     117\n",
       "   services          224\n",
       "   student           166\n",
       "   technician        478\n",
       "   unemployed        130\n",
       "   unknown            18\n",
       "Name: job, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： marital\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  marital \n",
       "0  divorced     2691\n",
       "   married     14744\n",
       "   single       6519\n",
       "1  divorced      364\n",
       "   married      1667\n",
       "   single       1143\n",
       "Name: marital, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： education\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  education\n",
       "0  primary       3798\n",
       "   secondary    12382\n",
       "   tertiary      6790\n",
       "   unknown        984\n",
       "1  primary        352\n",
       "   secondary     1500\n",
       "   tertiary      1169\n",
       "   unknown        153\n",
       "Name: education, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  default\n",
       "0  no         23507\n",
       "   yes          447\n",
       "1  no          3137\n",
       "   yes           37\n",
       "Name: default, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： housing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  housing\n",
       "0  no          9991\n",
       "   yes        13963\n",
       "1  no          2012\n",
       "   yes         1162\n",
       "Name: housing, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： loan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  loan\n",
       "0  no      19912\n",
       "   yes      4042\n",
       "1  no       2876\n",
       "   yes       298\n",
       "Name: loan, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： contact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  contact  \n",
       "0  cellular     14951\n",
       "   telephone     1457\n",
       "   unknown       7546\n",
       "1  cellular      2629\n",
       "   telephone      230\n",
       "   unknown        315\n",
       "Name: contact, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  day\n",
       "0  1      138\n",
       "   2      675\n",
       "   3      557\n",
       "   4      739\n",
       "   5      997\n",
       "         ... \n",
       "1  27      83\n",
       "   28      81\n",
       "   29      73\n",
       "   30     166\n",
       "   31      28\n",
       "Name: day, Length: 62, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： month\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  month\n",
       "0  1         762\n",
       "   2        1328\n",
       "   3         147\n",
       "   4        1418\n",
       "   5        7751\n",
       "   6        2880\n",
       "   7        3760\n",
       "   8        3293\n",
       "   9         195\n",
       "   10        247\n",
       "   11       2107\n",
       "   12         66\n",
       "1  1          84\n",
       "   2         258\n",
       "   3         152\n",
       "   4         337\n",
       "   5         566\n",
       "   6         324\n",
       "   7         376\n",
       "   8         425\n",
       "   9         161\n",
       "   10        192\n",
       "   11        235\n",
       "   12         64\n",
       "Name: month, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： poutcome\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  poutcome\n",
       "0  failure      2578\n",
       "   other         950\n",
       "   success       312\n",
       "   unknown     20114\n",
       "1  failure       391\n",
       "   other         173\n",
       "   success       574\n",
       "   unknown      2036\n",
       "Name: poutcome, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  y\n",
       "0  0    23954\n",
       "1  1     3174\n",
       "Name: y, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： age_round\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  age_round\n",
       "0  10             21\n",
       "   20           2585\n",
       "   30           9674\n",
       "   40           6347\n",
       "   50           4636\n",
       "   60            283\n",
       "   70            408\n",
       "1  10             11\n",
       "   20            537\n",
       "   30           1162\n",
       "   40            637\n",
       "   50            466\n",
       "   60             64\n",
       "   70            297\n",
       "Name: age_round, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_col： age_round2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y  age_round2\n",
       "0  10              21\n",
       "   20            2585\n",
       "   30            9674\n",
       "   40            6347\n",
       "   50            4636\n",
       "   60             283\n",
       "   70             408\n",
       "1  10              11\n",
       "   20             537\n",
       "   30            1162\n",
       "   40             637\n",
       "   50             466\n",
       "   60              64\n",
       "   70             297\n",
       "Name: age_round2, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#categoricalについては目的変数でグルーピングしてカウント\n",
    "for target_col in categorical_col_list:\n",
    "    print(\"\\ntarget_col：\",target_col)\n",
    "    display(train_df.groupby([\"y\",target_col])[target_col].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>housing</th>\n",
       "      <th>no</th>\n",
       "      <th>yes</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.832375</td>\n",
       "      <td>0.923174</td>\n",
       "      <td>0.882999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.167625</td>\n",
       "      <td>0.076826</td>\n",
       "      <td>0.117001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "housing        no       yes       All\n",
       "y                                    \n",
       "0        0.832375  0.923174  0.882999\n",
       "1        0.167625  0.076826  0.117001"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(train_df['y'], train_df['housing'], margins=True, normalize='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#平均値と中央値を可視化して確認\n",
    "#yの01別で平均値、中央値\n",
    "y0_df = train_df.query('y == 0')\n",
    "y1_df = train_df.query('y == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_df2 = train_df[train_df['y'] == 0]\n",
    "y1_df2 = train_df[train_df['y'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "      <th>age_round</th>\n",
       "      <th>age_round2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>admin.</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>89</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>112</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>67</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>708</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>admin.</td>\n",
       "      <td>divorced</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>420</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>158</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>354</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27123</th>\n",
       "      <td>27124</td>\n",
       "      <td>42</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1455</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>279</td>\n",
       "      <td>2</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27124</th>\n",
       "      <td>27125</td>\n",
       "      <td>34</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>719</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>362</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27125</th>\n",
       "      <td>27126</td>\n",
       "      <td>36</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>single</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>49</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27126</th>\n",
       "      <td>27127</td>\n",
       "      <td>33</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>209</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27127</th>\n",
       "      <td>27128</td>\n",
       "      <td>29</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>40</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>355</td>\n",
       "      <td>2</td>\n",
       "      <td>failure</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23954 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  age            job   marital  education default  balance  \\\n",
       "19        20   29         admin.    single   tertiary      no       89   \n",
       "20        21   28  self-employed   married   tertiary      no      112   \n",
       "21        22   67        retired   married    primary      no      708   \n",
       "23        24   33         admin.  divorced  secondary      no      420   \n",
       "24        25   39       services    single  secondary      no      158   \n",
       "...      ...  ...            ...       ...        ...     ...      ...   \n",
       "27123  27124   42    blue-collar   married    primary      no     1455   \n",
       "27124  27125   34       services    single  secondary      no      719   \n",
       "27125  27126   36    blue-collar    single    primary      no       49   \n",
       "27126  27127   33      housemaid   married    primary      no      209   \n",
       "27127  27128   29       services    single  secondary      no       40   \n",
       "\n",
       "      housing loan    contact  day  month  duration  campaign  pdays  \\\n",
       "19        yes   no   cellular   22      7       120         1     -1   \n",
       "20        yes  yes   cellular    5      5       145         1     -1   \n",
       "21         no   no   cellular   11      9        96         2     -1   \n",
       "23        yes   no  telephone    6      2       112         2     -1   \n",
       "24        yes   no    unknown    6      6       354         2     -1   \n",
       "...       ...  ...        ...  ...    ...       ...       ...    ...   \n",
       "27123     yes   no   cellular    2      2       279         2    269   \n",
       "27124     yes   no    unknown   15      5       362         3     -1   \n",
       "27125     yes   no    unknown    8      5       405         1     -1   \n",
       "27126      no   no   cellular   12      8        76         4     -1   \n",
       "27127     yes   no   cellular   18      5       105         1    355   \n",
       "\n",
       "       previous poutcome  y  age_round  age_round2  \n",
       "19            0  unknown  0         20          20  \n",
       "20            0  unknown  0         20          20  \n",
       "21            0  unknown  0         70          70  \n",
       "23            0  unknown  0         30          30  \n",
       "24            0  unknown  0         30          30  \n",
       "...         ...      ... ..        ...         ...  \n",
       "27123         2    other  0         40          40  \n",
       "27124         0  unknown  0         30          30  \n",
       "27125         0  unknown  0         30          30  \n",
       "27126         0  unknown  0         30          30  \n",
       "27127         2  failure  0         20          20  \n",
       "\n",
       "[23954 rows x 20 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "      <th>age_round</th>\n",
       "      <th>age_round2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>admin.</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>89</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>112</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>67</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>708</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>admin.</td>\n",
       "      <td>divorced</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>420</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>158</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>354</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27123</th>\n",
       "      <td>27124</td>\n",
       "      <td>42</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1455</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>279</td>\n",
       "      <td>2</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27124</th>\n",
       "      <td>27125</td>\n",
       "      <td>34</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>719</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>362</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27125</th>\n",
       "      <td>27126</td>\n",
       "      <td>36</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>single</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>49</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27126</th>\n",
       "      <td>27127</td>\n",
       "      <td>33</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>209</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27127</th>\n",
       "      <td>27128</td>\n",
       "      <td>29</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>40</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>355</td>\n",
       "      <td>2</td>\n",
       "      <td>failure</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23954 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  age            job   marital  education default  balance  \\\n",
       "19        20   29         admin.    single   tertiary      no       89   \n",
       "20        21   28  self-employed   married   tertiary      no      112   \n",
       "21        22   67        retired   married    primary      no      708   \n",
       "23        24   33         admin.  divorced  secondary      no      420   \n",
       "24        25   39       services    single  secondary      no      158   \n",
       "...      ...  ...            ...       ...        ...     ...      ...   \n",
       "27123  27124   42    blue-collar   married    primary      no     1455   \n",
       "27124  27125   34       services    single  secondary      no      719   \n",
       "27125  27126   36    blue-collar    single    primary      no       49   \n",
       "27126  27127   33      housemaid   married    primary      no      209   \n",
       "27127  27128   29       services    single  secondary      no       40   \n",
       "\n",
       "      housing loan    contact  day  month  duration  campaign  pdays  \\\n",
       "19        yes   no   cellular   22      7       120         1     -1   \n",
       "20        yes  yes   cellular    5      5       145         1     -1   \n",
       "21         no   no   cellular   11      9        96         2     -1   \n",
       "23        yes   no  telephone    6      2       112         2     -1   \n",
       "24        yes   no    unknown    6      6       354         2     -1   \n",
       "...       ...  ...        ...  ...    ...       ...       ...    ...   \n",
       "27123     yes   no   cellular    2      2       279         2    269   \n",
       "27124     yes   no    unknown   15      5       362         3     -1   \n",
       "27125     yes   no    unknown    8      5       405         1     -1   \n",
       "27126      no   no   cellular   12      8        76         4     -1   \n",
       "27127     yes   no   cellular   18      5       105         1    355   \n",
       "\n",
       "       previous poutcome  y  age_round  age_round2  \n",
       "19            0  unknown  0         20          20  \n",
       "20            0  unknown  0         20          20  \n",
       "21            0  unknown  0         70          70  \n",
       "23            0  unknown  0         30          30  \n",
       "24            0  unknown  0         30          30  \n",
       "...         ...      ... ..        ...         ...  \n",
       "27123         2    other  0         40          40  \n",
       "27124         0  unknown  0         30          30  \n",
       "27125         0  unknown  0         30          30  \n",
       "27126         0  unknown  0         30          30  \n",
       "27127         2  failure  0         20          20  \n",
       "\n",
       "[23954 rows x 20 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows' ,500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target_col in numeric_col_list:\n",
    "\n",
    "    #0,1別平均値\n",
    "    y0_target_col_mean = y0_df[target_col].mean()\n",
    "    y1_target_col_mean = y1_df[target_col].mean()\n",
    "\n",
    "    #0,1別中央値\n",
    "    y0_target_col_median = y0_df[target_col].median()\n",
    "    y1_target_col_median = y1_df[target_col].median()\n",
    "\n",
    "    #縦軸設定\n",
    "    #平均値、中央値のうち最大値の1.1倍を縦軸とする\n",
    "    #倍率は見やすければ何でもよい\n",
    "    graph_y_length = (1.1*max(y0_target_col_mean,y1_target_col_mean,y0_target_col_median,y1_target_col_median))\n",
    "\n",
    "    plt.title(target_col + \": mean\")\n",
    "    plt.ylabel(target_col)\n",
    "    plt.ylim([0,graph_y_length],)\n",
    "    plt.bar([\"y0_mean\",\"y1_mean\"],[y0_target_col_mean,y1_target_col_mean])\n",
    "    #plt.annotate(y0_target_col_mean,y0_target_col_mean)\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(target_col + \": median\")\n",
    "    plt.ylabel(target_col)\n",
    "    plt.ylim([0,graph_y_length],)\n",
    "    plt.bar([\"y0_median\",\"y1_median\"],[y0_target_col_median,y1_target_col_median],color = \"green\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y0 age\n",
      "y1 age\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y0 balance\n",
      "y1 balance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y0 duration\n",
      "y1 duration\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y0 campaign\n",
      "y1 campaign\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y0 pdays\n",
      "y1 pdays\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y0 previous\n",
      "y1 previous\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ヒストグラムでも全体像把握\n",
    "for target_col in numeric_col_list:\n",
    "    #軸の設定\n",
    "    graph_x_length = 1.1*max(train_df[target_col])\n",
    "    graph_y_length = len(y0_df)\n",
    "\n",
    "    print(\"y0\",target_col)\n",
    "    plt.xlim([0,graph_x_length],)\n",
    "    plt.ylim([0,graph_y_length],)\n",
    "    plt.hist(y0_df[target_col],bins = 20, alpha=0.3, color='blue')\n",
    "#     plt.show()\n",
    "\n",
    "    graph_y_length = graph_y_length/10\n",
    "    print(\"y1\",target_col)\n",
    "    plt.xlim([0,graph_x_length],)\n",
    "    plt.ylim([0,graph_y_length],)\n",
    "    plt.hist(y1_df[target_col],bins = 20, alpha=0.3, color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>y</th>\n",
       "      <th>age_round</th>\n",
       "      <th>age_round2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005716</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>-0.004526</td>\n",
       "      <td>-0.005425</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>-0.006858</td>\n",
       "      <td>-0.006858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.005716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095343</td>\n",
       "      <td>-0.008518</td>\n",
       "      <td>0.091014</td>\n",
       "      <td>-0.005309</td>\n",
       "      <td>-0.001340</td>\n",
       "      <td>-0.025272</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>0.958293</td>\n",
       "      <td>0.958293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.095343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.094794</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>-0.016295</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.045826</td>\n",
       "      <td>0.095728</td>\n",
       "      <td>0.095728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0.002974</td>\n",
       "      <td>-0.008518</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098321</td>\n",
       "      <td>-0.032453</td>\n",
       "      <td>0.164880</td>\n",
       "      <td>-0.096889</td>\n",
       "      <td>-0.050009</td>\n",
       "      <td>-0.031058</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>-0.006763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.091014</td>\n",
       "      <td>0.094794</td>\n",
       "      <td>0.098321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014608</td>\n",
       "      <td>0.057231</td>\n",
       "      <td>-0.113627</td>\n",
       "      <td>-0.038063</td>\n",
       "      <td>0.022353</td>\n",
       "      <td>0.098894</td>\n",
       "      <td>0.098894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>0.002705</td>\n",
       "      <td>-0.005309</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>-0.032453</td>\n",
       "      <td>-0.014608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.087771</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.401390</td>\n",
       "      <td>-0.002985</td>\n",
       "      <td>-0.002985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>0.016867</td>\n",
       "      <td>-0.001340</td>\n",
       "      <td>-0.016295</td>\n",
       "      <td>0.164880</td>\n",
       "      <td>0.057231</td>\n",
       "      <td>-0.087771</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.086220</td>\n",
       "      <td>-0.031557</td>\n",
       "      <td>-0.076118</td>\n",
       "      <td>-0.001201</td>\n",
       "      <td>-0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdays</th>\n",
       "      <td>-0.004526</td>\n",
       "      <td>-0.025272</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>-0.096889</td>\n",
       "      <td>-0.113627</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>-0.086220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421606</td>\n",
       "      <td>0.100930</td>\n",
       "      <td>-0.020755</td>\n",
       "      <td>-0.020755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous</th>\n",
       "      <td>-0.005425</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>-0.050009</td>\n",
       "      <td>-0.038063</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>-0.031557</td>\n",
       "      <td>0.421606</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083825</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.006822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.003555</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>0.045826</td>\n",
       "      <td>-0.031058</td>\n",
       "      <td>0.022353</td>\n",
       "      <td>0.401390</td>\n",
       "      <td>-0.076118</td>\n",
       "      <td>0.100930</td>\n",
       "      <td>0.083825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031063</td>\n",
       "      <td>0.031063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_round</th>\n",
       "      <td>-0.006858</td>\n",
       "      <td>0.958293</td>\n",
       "      <td>0.095728</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>0.098894</td>\n",
       "      <td>-0.002985</td>\n",
       "      <td>-0.001201</td>\n",
       "      <td>-0.020755</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.031063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_round2</th>\n",
       "      <td>-0.006858</td>\n",
       "      <td>0.958293</td>\n",
       "      <td>0.095728</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>0.098894</td>\n",
       "      <td>-0.002985</td>\n",
       "      <td>-0.001201</td>\n",
       "      <td>-0.020755</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.031063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id       age   balance       day     month  duration  \\\n",
       "id          1.000000 -0.005716 -0.000484  0.002974  0.001007  0.002705   \n",
       "age        -0.005716  1.000000  0.095343 -0.008518  0.091014 -0.005309   \n",
       "balance    -0.000484  0.095343  1.000000  0.002067  0.094794  0.019923   \n",
       "day         0.002974 -0.008518  0.002067  1.000000  0.098321 -0.032453   \n",
       "month       0.001007  0.091014  0.094794  0.098321  1.000000 -0.014608   \n",
       "duration    0.002705 -0.005309  0.019923 -0.032453 -0.014608  1.000000   \n",
       "campaign    0.016867 -0.001340 -0.016295  0.164880  0.057231 -0.087771   \n",
       "pdays      -0.004526 -0.025272  0.003613 -0.096889 -0.113627  0.002030   \n",
       "previous   -0.005425  0.002946  0.012483 -0.050009 -0.038063  0.002489   \n",
       "y          -0.003555  0.020892  0.045826 -0.031058  0.022353  0.401390   \n",
       "age_round  -0.006858  0.958293  0.095728 -0.006763  0.098894 -0.002985   \n",
       "age_round2 -0.006858  0.958293  0.095728 -0.006763  0.098894 -0.002985   \n",
       "\n",
       "            campaign     pdays  previous         y  age_round  age_round2  \n",
       "id          0.016867 -0.004526 -0.005425 -0.003555  -0.006858   -0.006858  \n",
       "age        -0.001340 -0.025272  0.002946  0.020892   0.958293    0.958293  \n",
       "balance    -0.016295  0.003613  0.012483  0.045826   0.095728    0.095728  \n",
       "day         0.164880 -0.096889 -0.050009 -0.031058  -0.006763   -0.006763  \n",
       "month       0.057231 -0.113627 -0.038063  0.022353   0.098894    0.098894  \n",
       "duration   -0.087771  0.002030  0.002489  0.401390  -0.002985   -0.002985  \n",
       "campaign    1.000000 -0.086220 -0.031557 -0.076118  -0.001201   -0.001201  \n",
       "pdays      -0.086220  1.000000  0.421606  0.100930  -0.020755   -0.020755  \n",
       "previous   -0.031557  0.421606  1.000000  0.083825   0.006822    0.006822  \n",
       "y          -0.076118  0.100930  0.083825  1.000000   0.031063    0.031063  \n",
       "age_round  -0.001201 -0.020755  0.006822  0.031063   1.000000    1.000000  \n",
       "age_round2 -0.001201 -0.020755  0.006822  0.031063   1.000000    1.000000  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#相関係数算出\n",
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           -0.003555\n",
       "age           0.020892\n",
       "balance       0.045826\n",
       "day          -0.031058\n",
       "month         0.022353\n",
       "duration      0.401390\n",
       "campaign     -0.076118\n",
       "pdays         0.100930\n",
       "previous      0.083825\n",
       "y             1.000000\n",
       "age_round     0.031063\n",
       "age_round2    0.031063\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#目的変数の相関係数だけピックアップ\n",
    "train_df.corr()[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
